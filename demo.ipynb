{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from HM_LSTM_Cell import HM_LSTM_Cell\n",
    "from Multi_HM_LSTM_Cell import Multi_HM_LSTM_Cell\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from IPython.display import clear_output\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### HYPERPARAMS #######\n",
    "batch_size = 40\n",
    "J = 100\n",
    "emb_dim = 128\n",
    "hidden_dim = 350\n",
    "num_layers = 3\n",
    "\n",
    "learning_rate = 0.0013\n",
    "grad_clip = 5.0\n",
    "\n",
    "init_slope_value = 1.0\n",
    "max_slope = 5.0\n",
    "slope_annealing_increase_per_epoch = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hosh(x):\n",
    "    hv = 0\n",
    "    for c in list(str(x)):\n",
    "        hv = hv ^ ord(c)\n",
    "    return hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### DATASET PREP #######\n",
    "config = configparser.ConfigParser()\n",
    "config.read('.config')\n",
    "\n",
    "fp = config['default']['fp']\n",
    "label_colname = config['default']['label_colname']\n",
    "text_colname = config['default']['text_colname']\n",
    "\n",
    "provider_column_names = [label_colname, text_colname]\n",
    "provider_label_values_hashed = [8, 70]\n",
    "\n",
    "df = pd.read_csv(fp, sep='\\t', names=provider_column_names, skiprows=1, quoting=csv.QUOTE_NONE, quotechar='|', escapechar='\\\\')\n",
    "df = df[df[label_colname].apply(lambda x: hosh(x) in provider_label_values_hashed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = df[text_colname].tolist()\n",
    "counter = Counter()\n",
    "\n",
    "for comment in comments:\n",
    "    counter.update(list(comment))\n",
    "\n",
    "vocab = [k for k, v in counter.items() if v >= 20]\n",
    "\n",
    "go = '\\x00'\n",
    "end_of_text = '\\x01'\n",
    "pad = '\\x02'\n",
    "end_of_padded_comment = '\\x03'\n",
    "unk = '\\x04'\n",
    "\n",
    "vocab.append(go)\n",
    "vocab.append(end_of_text)\n",
    "vocab.append(pad)\n",
    "vocab.append(end_of_padded_comment)\n",
    "vocab.append(unk)\n",
    "\n",
    "int2char = {i: c for i, c in enumerate(vocab)}\n",
    "char2int = {c: i for i, c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_colname = 'clean_' + text_colname\n",
    "standardized_text_colname = 'standardized_' + text_colname\n",
    "\n",
    "def clean(x):\n",
    "    return ''.join([c for c in x if ord(c) < 128])\n",
    "\n",
    "def standardize(x):\n",
    "    token_list = []\n",
    "    token_list.append(go)\n",
    "    token_list.extend(list(x))\n",
    "    token_list.append(end_of_text)\n",
    "    token_list.extend([pad for _ in range(0, max(0, J-len(x)-2))])\n",
    "    token_list.append(end_of_padded_comment)\n",
    "    return ''.join(token_list)[0:(J+1)]\n",
    "\n",
    "def filter_cond(x):\n",
    "    if len(x) + 3 <= J:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def token2int(x):\n",
    "    return [(char2int[c] if c in char2int else char2int[unk]) for c in x]\n",
    "\n",
    "df[clean_text_colname] = df[text_colname].apply(clean)\n",
    "\n",
    "df_filtered = df.copy(deep=True)\n",
    "df_filtered = df_filtered[df_filtered[clean_text_colname].apply(filter_cond)]\n",
    "df_filtered[standardized_text_colname] = df_filtered[clean_text_colname].apply(standardize)\n",
    "df_filtered['comment_ints'] = df_filtered[standardized_text_colname].apply(token2int)\n",
    "\n",
    "nr_filtered_provider_records = df_filtered.shape[0]\n",
    "dataset_size = batch_size * (nr_filtered_provider_records // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = len(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "\n",
      "input chars:\n",
      "[81 37 34 76 76 57 44 56 11 50 76 34 60 55 40 60 21 63 11 49 13 57 34 76\n",
      " 63 11 42 44 40 35 49 11 63 77 49 11 34 63 11 57 11  2 40 44 44 26 42 55\n",
      " 11 39 76 40 14 57 76 34 63 55 11 44 34 63 40 55 55 40 69 49 57 55 49 44\n",
      " 73 73 82 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83\n",
      " 83 83 83 83]\n",
      "\n",
      "predict chars:\n",
      "[37 34 76 76 57 44 56 11 50 76 34 60 55 40 60 21 63 11 49 13 57 34 76 63\n",
      " 11 42 44 40 35 49 11 63 77 49 11 34 63 11 57 11  2 40 44 44 26 42 55 11\n",
      " 39 76 40 14 57 76 34 63 55 11 44 34 63 40 55 55 40 69 49 57 55 49 44 73\n",
      " 73 82 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83 83\n",
      " 83 83 83 84]\n"
     ]
    }
   ],
   "source": [
    "####### Dataset format - inspect #######\n",
    "comment_ints_batch = df_filtered['comment_ints'].iloc[0:batch_size].tolist()\n",
    "comment_ints_batch = np.array(comment_ints_batch, dtype=np.int32)\n",
    "xs_batch = comment_ints_batch[:,0:J]\n",
    "ys_batch = comment_ints_batch[:,1:(J+1)]\n",
    "print(\"Example:\\n\")\n",
    "print(\"input chars:\")\n",
    "print(xs_batch[0,:])\n",
    "print(\"\\npredict chars:\")\n",
    "print(ys_batch[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### HM-LSTM #######\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, batch_size, J, V, emb_dim, hidden_dim, num_layers, \n",
    "                 learning_rate, grad_clip, sampling):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        if sampling == True:\n",
    "            batch_size, J = 1, 1\n",
    "        else:\n",
    "            batch_size, J = batch_size, J\n",
    "        \n",
    "        self.V = V\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grad_clip = grad_clip\n",
    "            \n",
    "        self.slope_annealing_placeholder = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "        self.xs = tf.placeholder(tf.int32, [batch_size, J])\n",
    "        self.ys = tf.placeholder(tf.int32, [batch_size, J])\n",
    "        \n",
    "        self.emb_mat = tf.get_variable(\n",
    "            name='emb_mat', dtype=tf.float32, shape=[self.V, self.emb_dim])\n",
    "        \n",
    "        self.emb_xs = tf.nn.embedding_lookup(self.emb_mat, self.xs)\n",
    "        #self.emb_xs = tf.one_hot(self.xs, self.V)\n",
    "\n",
    "        self.multi_cell = Multi_HM_LSTM_Cell(\n",
    "            [HM_LSTM_Cell(\n",
    "                num_units=self.hidden_dim, \n",
    "                slope_annealing_placeholder=self.slope_annealing_placeholder,\n",
    "                forget_bias=1.0)\n",
    "             for _ in range(0, self.num_layers)]\n",
    "        )\n",
    "        '''\n",
    "        self.multi_cell = tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.contrib.rnn.BasicLSTMCell(num_units=self.hidden_dim, forget_bias=1.0) \n",
    "             for _ in range(0, self.num_layers)]\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        self.initial_state = self.multi_cell.zero_state(batch_size, tf.float32)\n",
    "        self.outputs, self.state = tf.nn.dynamic_rnn(\n",
    "            cell=self.multi_cell, inputs=self.emb_xs, initial_state=self.initial_state)\n",
    "\n",
    "        h_layer1, h_layer2, h_layer3 = self.outputs\n",
    "\n",
    "        print(\"each h_layer output should have shape [batch_size, timesteps, hidden dim]\")\n",
    "        print(h_layer1.get_shape().as_list())\n",
    "        print(h_layer2.get_shape().as_list())\n",
    "        print(h_layer3.get_shape().as_list())\n",
    "\n",
    "        h_layer1_per_char = tf.reshape(h_layer1, [-1, self.hidden_dim])\n",
    "        h_layer2_per_char = tf.reshape(h_layer2, [-1, self.hidden_dim])\n",
    "        h_layer3_per_char = tf.reshape(h_layer3, [-1, self.hidden_dim])\n",
    "\n",
    "        h_out_per_char = tf.concat([h_layer1_per_char, h_layer2_per_char, h_layer3_per_char], 1)\n",
    "\n",
    "        g1 = tf.layers.dense(h_out_per_char, units=1, use_bias=False, activation=tf.nn.sigmoid)\n",
    "        g2 = tf.layers.dense(h_out_per_char, units=1, use_bias=False, activation=tf.nn.sigmoid)\n",
    "        g3 = tf.layers.dense(h_out_per_char, units=1, use_bias=False, activation=tf.nn.sigmoid)\n",
    "\n",
    "        self.output_emb = tf.layers.dense(\n",
    "            tf.concat([\n",
    "                g1 * h_layer1_per_char, \n",
    "                g2 * h_layer2_per_char, \n",
    "                g3 * h_layer3_per_char\n",
    "            ], 1), \n",
    "            units=self.hidden_dim, \n",
    "            use_bias=False, \n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        input_to_logits = self.output_emb\n",
    "        very_fancy_dim = self.num_layers * self.hidden_dim\n",
    "        '''\n",
    "        very_fancy_dim = hidden_dim\n",
    "        '''\n",
    "\n",
    "        very_fancy_output = tf.reshape(self.outputs, [-1, very_fancy_dim])\n",
    "        with tf.variable_scope('softmax'):\n",
    "            softmax_w = tf.Variable(tf.truncated_normal((very_fancy_dim, self.V), stddev=0.1))\n",
    "    \n",
    "        self.logits = tf.matmul(very_fancy_output, softmax_w)\n",
    "        self.probabilities = tf.nn.softmax(self.logits)\n",
    "\n",
    "        self.loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels=self.ys,\n",
    "            logits=tf.reshape(self.logits, self.ys.get_shape().as_list() + [V])\n",
    "        )\n",
    "\n",
    "        #train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        tvars = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        gradients, _ = zip(*optimizer.compute_gradients(self.loss, tvars))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, grad_clip)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each h_layer output should have shape [batch_size, timesteps, hidden dim]\n",
      "[40, 100, 350]\n",
      "[40, 100, 350]\n",
      "[40, 100, 350]\n",
      "Epoch 0 / 10... step 0...\t training loss: 4.4473557472229\n",
      "Epoch 0 / 10... step 10...\t training loss: 4.994361400604248\n",
      "Epoch 0 / 10... step 20...\t training loss: 2.7366459369659424\n",
      "Epoch 0 / 10... step 30...\t training loss: 2.3921823501586914\n",
      "Epoch 0 / 10... step 40...\t training loss: 2.514673948287964\n",
      "Epoch 0 / 10... step 50...\t training loss: 2.5279386043548584\n",
      "Epoch 0 / 10... step 60...\t training loss: 2.4569594860076904\n",
      "Epoch 0 / 10... step 70...\t training loss: 2.4191956520080566\n",
      "Epoch 0 / 10... step 80...\t training loss: 2.478078603744507\n",
      "Epoch 0 / 10... step 90...\t training loss: 2.70499587059021\n",
      "Epoch 0 / 10... step 100...\t training loss: 2.4354286193847656\n",
      "Epoch 0 / 10... step 110...\t training loss: 2.3667025566101074\n",
      "Epoch 0 / 10... step 120...\t training loss: 2.58023738861084\n"
     ]
    }
   ],
   "source": [
    "nr_epochs = 10\n",
    "\n",
    "model = Model(batch_size, J, V, emb_dim, hidden_dim, num_layers, \n",
    "    learning_rate, grad_clip, sampling=False)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "for epoch in range(0, nr_epochs):\n",
    "    slope_value = min(max_slope, init_slope_value + slope_annealing_increase_per_epoch * epoch)\n",
    "    df_filtered = df_filtered.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    for i in range(0, dataset_size // batch_size):\n",
    "        batch_idx = i * batch_size\n",
    "\n",
    "        comment_ints_batch = df_filtered['comment_ints'].iloc[batch_idx:batch_idx+batch_size].tolist()\n",
    "        comment_ints_batch = np.array(comment_ints_batch, dtype=np.int32)\n",
    "\n",
    "        xs_batch = comment_ints_batch[:,0:J]\n",
    "        ys_batch = comment_ints_batch[:,1:(J+1)]\n",
    "\n",
    "        feed_dict = {model.xs: xs_batch, \n",
    "                     model.ys: ys_batch, \n",
    "                     model.slope_annealing_placeholder: slope_value}\n",
    "\n",
    "        _, loss_batch = sess.run(\n",
    "            [model.train_op, model.loss], feed_dict=feed_dict\n",
    "        )\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(\"Epoch {} / {}... step {}...\\t training loss: {}\".format(\n",
    "                epoch, nr_epochs, i, loss_batch))\n",
    "        \n",
    "        if epoch > 0 and (epoch % 10 == 0):\n",
    "            clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
