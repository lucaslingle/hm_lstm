{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from HM_LSTM_Cell import HM_LSTM_Cell\n",
    "from Multi_HM_LSTM_Cell import Multi_HM_LSTM_Cell\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from IPython.display import clear_output\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### HYPERPARAMS #######\n",
    "batch_size = 40\n",
    "J = 100\n",
    "emb_dim = 128\n",
    "hidden_dim = 350\n",
    "num_layers = 3\n",
    "\n",
    "learning_rate = 0.002\n",
    "grad_clip = 5.0\n",
    "\n",
    "init_slope_value = 1.0\n",
    "max_slope = 5.0\n",
    "slope_annealing_increase_per_epoch = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hosh(x):\n",
    "    hv = 0\n",
    "    for c in list(str(x)):\n",
    "        hv = hv ^ ord(c)\n",
    "    return hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### DATASET PREP #######\n",
    "config = configparser.ConfigParser()\n",
    "config.read('.config')\n",
    "\n",
    "fp = config['default']['fp']\n",
    "label_colname = config['default']['label_colname']\n",
    "text_colname = config['default']['text_colname']\n",
    "\n",
    "provider_column_names = [label_colname, text_colname]\n",
    "provider_label_values_hashed = [8, 70]\n",
    "\n",
    "df = pd.read_csv(fp, sep='\\t', names=provider_column_names, skiprows=1, quoting=csv.QUOTE_NONE, quotechar='|', escapechar='\\\\')\n",
    "df = df[df[label_colname].apply(lambda x: hosh(x) in provider_label_values_hashed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    return ''.join([c for c in x if ord(c) < 128])\n",
    "\n",
    "clean_text_colname = 'clean_' + text_colname\n",
    "df[clean_text_colname] = df[text_colname].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_cond(x):\n",
    "    if len(x) + 3 <= J:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df_filtered = df.copy(deep=True)\n",
    "df_filtered = df_filtered[df_filtered[clean_text_colname].apply(filter_cond)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_counter = Counter()\n",
    "comments = df_filtered[clean_text_colname].tolist()\n",
    "\n",
    "for comment_text in comments:\n",
    "    comment_chars = list(comment_text)\n",
    "    char_counter.update(comment_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 47490),\n",
       " ('e', 22918),\n",
       " ('t', 16863),\n",
       " ('a', 15616),\n",
       " ('o', 15240),\n",
       " ('i', 14160),\n",
       " ('s', 13762),\n",
       " ('n', 13133),\n",
       " ('r', 10814),\n",
       " ('h', 10299),\n",
       " ('l', 9264),\n",
       " ('u', 6812),\n",
       " ('d', 6590),\n",
       " ('m', 5254),\n",
       " ('.', 4949),\n",
       " ('c', 4777),\n",
       " ('y', 4611),\n",
       " ('p', 4214),\n",
       " ('g', 4094),\n",
       " ('w', 4011),\n",
       " ('f', 3261),\n",
       " ('b', 3033),\n",
       " ('k', 2387),\n",
       " ('T', 2040),\n",
       " ('v', 1936),\n",
       " ('!', 1740),\n",
       " ('I', 1692),\n",
       " ('S', 1421),\n",
       " (\"'\", 1345),\n",
       " ('A', 1332),\n",
       " ('L', 1116),\n",
       " ('N', 1104),\n",
       " ('O', 1087),\n",
       " (',', 1085),\n",
       " ('E', 991),\n",
       " ('H', 958),\n",
       " ('C', 848),\n",
       " ('?', 809),\n",
       " ('R', 801),\n",
       " ('D', 799),\n",
       " ('M', 746),\n",
       " ('P', 744),\n",
       " ('W', 721),\n",
       " ('B', 492),\n",
       " ('G', 457),\n",
       " ('j', 446),\n",
       " ('F', 406),\n",
       " ('Y', 392),\n",
       " ('U', 364),\n",
       " ('\"', 328),\n",
       " ('K', 320),\n",
       " ('x', 267),\n",
       " ('0', 255),\n",
       " ('-', 214),\n",
       " ('z', 212),\n",
       " ('J', 203),\n",
       " ('2', 190),\n",
       " (':', 171),\n",
       " ('V', 135),\n",
       " ('1', 128),\n",
       " (')', 93),\n",
       " ('3', 76),\n",
       " ('4', 66),\n",
       " ('q', 64),\n",
       " ('5', 56),\n",
       " ('#', 56),\n",
       " ('(', 49),\n",
       " ('/', 45),\n",
       " ('7', 40),\n",
       " ('9', 37),\n",
       " ('6', 36),\n",
       " ('8', 36),\n",
       " ('=', 34),\n",
       " ('%', 31),\n",
       " ('X', 29),\n",
       " ('Q', 24),\n",
       " ('*', 21),\n",
       " ('&', 19),\n",
       " (';', 19),\n",
       " ('Z', 13),\n",
       " ('>', 11),\n",
       " ('~', 10),\n",
       " ('@', 9),\n",
       " ('$', 7),\n",
       " ('<', 6),\n",
       " ('^', 6),\n",
       " ('+', 3),\n",
       " ('_', 2),\n",
       " ('[', 1),\n",
       " (']', 1),\n",
       " ('`', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = [k for k, v in char_counter.items() if v > 20]\n",
    "\n",
    "go = '\\x00'\n",
    "end_of_text = '\\x01'\n",
    "pad = '\\x02'\n",
    "end_of_padded_comment = '\\x03'\n",
    "unk = '\\x04'\n",
    "\n",
    "vocab.append(go)\n",
    "vocab.append(end_of_text)\n",
    "vocab.append(pad)\n",
    "vocab.append(end_of_padded_comment)\n",
    "vocab.append(unk)\n",
    "\n",
    "vocab = sorted(vocab, key=lambda c: ord(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int2char = {i: c for i, c in enumerate(vocab)}\n",
    "char2int = {c: i for i, c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\x00': 0,\n",
       " '\\x01': 1,\n",
       " '\\x02': 2,\n",
       " '\\x03': 3,\n",
       " '\\x04': 4,\n",
       " ' ': 5,\n",
       " '!': 6,\n",
       " '\"': 7,\n",
       " '#': 8,\n",
       " '%': 9,\n",
       " \"'\": 10,\n",
       " '(': 11,\n",
       " ')': 12,\n",
       " '*': 13,\n",
       " ',': 14,\n",
       " '-': 15,\n",
       " '.': 16,\n",
       " '/': 17,\n",
       " '0': 18,\n",
       " '1': 19,\n",
       " '2': 20,\n",
       " '3': 21,\n",
       " '4': 22,\n",
       " '5': 23,\n",
       " '6': 24,\n",
       " '7': 25,\n",
       " '8': 26,\n",
       " '9': 27,\n",
       " ':': 28,\n",
       " '=': 29,\n",
       " '?': 30,\n",
       " 'A': 31,\n",
       " 'B': 32,\n",
       " 'C': 33,\n",
       " 'D': 34,\n",
       " 'E': 35,\n",
       " 'F': 36,\n",
       " 'G': 37,\n",
       " 'H': 38,\n",
       " 'I': 39,\n",
       " 'J': 40,\n",
       " 'K': 41,\n",
       " 'L': 42,\n",
       " 'M': 43,\n",
       " 'N': 44,\n",
       " 'O': 45,\n",
       " 'P': 46,\n",
       " 'Q': 47,\n",
       " 'R': 48,\n",
       " 'S': 49,\n",
       " 'T': 50,\n",
       " 'U': 51,\n",
       " 'V': 52,\n",
       " 'W': 53,\n",
       " 'X': 54,\n",
       " 'Y': 55,\n",
       " 'a': 56,\n",
       " 'b': 57,\n",
       " 'c': 58,\n",
       " 'd': 59,\n",
       " 'e': 60,\n",
       " 'f': 61,\n",
       " 'g': 62,\n",
       " 'h': 63,\n",
       " 'i': 64,\n",
       " 'j': 65,\n",
       " 'k': 66,\n",
       " 'l': 67,\n",
       " 'm': 68,\n",
       " 'n': 69,\n",
       " 'o': 70,\n",
       " 'p': 71,\n",
       " 'q': 72,\n",
       " 'r': 73,\n",
       " 's': 74,\n",
       " 't': 75,\n",
       " 'u': 76,\n",
       " 'v': 77,\n",
       " 'w': 78,\n",
       " 'x': 79,\n",
       " 'y': 80,\n",
       " 'z': 81}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    token_list = []\n",
    "    token_list.append(go)\n",
    "    token_list.extend(list(x))\n",
    "    token_list.append(end_of_text)\n",
    "    token_list.extend([pad for _ in range(0, max(0, J-len(x)-2))])\n",
    "    token_list.append(end_of_padded_comment)\n",
    "    return ''.join(token_list)[0:(J+1)]\n",
    "\n",
    "standardized_text_colname = 'standardized_' + text_colname\n",
    "df_filtered[standardized_text_colname] = df_filtered[clean_text_colname].apply(standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token2int(x):\n",
    "    return [(char2int[c] if c in char2int else char2int[unk]) for c in x]\n",
    "\n",
    "comment_int_colname = 'comment_ints'\n",
    "df_filtered[comment_int_colname] = df_filtered[standardized_text_colname].apply(token2int)\n",
    "\n",
    "nr_filtered_provider_records = df_filtered.shape[0]\n",
    "dataset_size = batch_size * (nr_filtered_provider_records // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = len(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "\n",
      "input chars:\n",
      "[ 0 38 64 67 67 56 73 80  5 33 67 64 69 75 70 69 10 74  5 60 68 56 64 67\n",
      " 74  5 71 73 70 77 60  5 74 63 60  5 64 74  5 56  5 58 70 73 73 76 71 75\n",
      "  5 62 67 70 57 56 67 64 74 75  5 73 64 74 70 75 75 70 15 60 56 75 60 73\n",
      "  6  6  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2]\n",
      "\n",
      "predict chars:\n",
      "[38 64 67 67 56 73 80  5 33 67 64 69 75 70 69 10 74  5 60 68 56 64 67 74\n",
      "  5 71 73 70 77 60  5 74 63 60  5 64 74  5 56  5 58 70 73 73 76 71 75  5\n",
      " 62 67 70 57 56 67 64 74 75  5 73 64 74 70 75 75 70 15 60 56 75 60 73  6\n",
      "  6  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  3]\n"
     ]
    }
   ],
   "source": [
    "####### Dataset format - inspect #######\n",
    "comment_ints_batch = df_filtered[comment_int_colname].iloc[0:batch_size].tolist()\n",
    "comment_ints_batch = np.array(comment_ints_batch, dtype=np.int32)\n",
    "xs_batch = comment_ints_batch[:,0:J]\n",
    "ys_batch = comment_ints_batch[:,1:(J+1)]\n",
    "print(\"Example:\\n\")\n",
    "print(\"input chars:\")\n",
    "print(xs_batch[0,:])\n",
    "print(\"\\npredict chars:\")\n",
    "print(ys_batch[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### HM-LSTM #######\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, batch_size, J, V, emb_dim, hidden_dim, num_layers, \n",
    "                 learning_rate, grad_clip, sampling):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        if sampling == True:\n",
    "            batch_size, J = 1, 1\n",
    "        else:\n",
    "            batch_size, J = batch_size, J\n",
    "        \n",
    "        self.V = V\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grad_clip = grad_clip\n",
    "            \n",
    "        self.slope_annealing_placeholder = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "        self.xs = tf.placeholder(tf.int32, [batch_size, J])\n",
    "        self.ys = tf.placeholder(tf.int32, [batch_size, J])\n",
    "        \n",
    "        #_initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "        \n",
    "        self.emb_mat = tf.get_variable(\n",
    "            name='emb_mat', dtype=tf.float32, shape=[self.V, self.emb_dim])\n",
    "        \n",
    "        self.emb_xs = tf.nn.embedding_lookup(self.emb_mat, self.xs)\n",
    "        #self.emb_xs = tf.one_hot(self.xs, self.V)\n",
    "\n",
    "        self.multi_cell = Multi_HM_LSTM_Cell(\n",
    "            [HM_LSTM_Cell(\n",
    "                num_units=self.hidden_dim, \n",
    "                slope_annealing_placeholder=self.slope_annealing_placeholder,\n",
    "                forget_bias=1.0)\n",
    "             for _ in range(0, self.num_layers)]\n",
    "        )\n",
    "        '''\n",
    "        self.multi_cell = tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.contrib.rnn.BasicLSTMCell(num_units=self.hidden_dim, forget_bias=1.0) \n",
    "             for _ in range(0, self.num_layers)]\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        self.initial_state = self.multi_cell.zero_state(batch_size, tf.float32)\n",
    "        self.outputs, self.state = tf.nn.dynamic_rnn(\n",
    "            cell=self.multi_cell, inputs=self.emb_xs, initial_state=self.initial_state)\n",
    "\n",
    "        h_layer1, h_layer2, h_layer3 = self.outputs\n",
    "\n",
    "        print(\"each h_layer output should have shape [batch_size, timesteps, hidden dim]\")\n",
    "        print(h_layer1.get_shape().as_list())\n",
    "        print(h_layer2.get_shape().as_list())\n",
    "        print(h_layer3.get_shape().as_list())\n",
    "\n",
    "        h_layer1_per_char = tf.reshape(h_layer1, [-1, self.hidden_dim])\n",
    "        h_layer2_per_char = tf.reshape(h_layer2, [-1, self.hidden_dim])\n",
    "        h_layer3_per_char = tf.reshape(h_layer3, [-1, self.hidden_dim])\n",
    "\n",
    "        h_out_per_char = tf.concat(\n",
    "            [h_layer1_per_char, h_layer2_per_char, h_layer3_per_char], 1)\n",
    "\n",
    "        g1 = tf.layers.dense(h_out_per_char, units=1, use_bias=False, activation=tf.nn.sigmoid)\n",
    "        g2 = tf.layers.dense(h_out_per_char, units=1, use_bias=False, activation=tf.nn.sigmoid)\n",
    "        g3 = tf.layers.dense(h_out_per_char, units=1, use_bias=False, activation=tf.nn.sigmoid)\n",
    "\n",
    "        self.output_emb = tf.layers.dense(\n",
    "            tf.concat([\n",
    "                g1 * h_layer1_per_char, \n",
    "                g2 * h_layer2_per_char, \n",
    "                g3 * h_layer3_per_char\n",
    "            ], 1), \n",
    "            units=self.hidden_dim, \n",
    "            use_bias=False, \n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        very_fancy_dim = self.num_layers * self.hidden_dim\n",
    "        '''\n",
    "        very_fancy_dim = hidden_dim\n",
    "        '''\n",
    "\n",
    "        very_fancy_output = tf.reshape(self.outputs, [-1, very_fancy_dim])\n",
    "        \n",
    "        with tf.variable_scope('logit_layer'):\n",
    "            logits_kernel = tf.get_variable(name='logits_kernel', \n",
    "                shape=[very_fancy_dim, self.V])\n",
    "    \n",
    "        self.logits = tf.matmul(very_fancy_output, logits_kernel)\n",
    "        self.probabilities = tf.nn.softmax(self.logits)\n",
    "\n",
    "        self.loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels=self.ys,\n",
    "            logits=tf.reshape(self.logits, self.ys.get_shape().as_list() + [V])\n",
    "        )\n",
    "\n",
    "        #self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        tvars = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        gradients, _ = zip(*optimizer.compute_gradients(self.loss, tvars))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, grad_clip)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each h_layer output should have shape [batch_size, timesteps, hidden dim]\n",
      "[40, 100, 350]\n",
      "[40, 100, 350]\n",
      "[40, 100, 350]\n"
     ]
    }
   ],
   "source": [
    "nr_epochs = 40\n",
    "\n",
    "model = Model(batch_size, J, V, emb_dim, hidden_dim, num_layers, \n",
    "    learning_rate, grad_clip, sampling=False)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 40... step 0...\t training loss: 4.407299041748047\n",
      "Epoch 0 / 40... step 1...\t training loss: 4.388210296630859\n",
      "Epoch 0 / 40... step 2...\t training loss: 4.308659553527832\n",
      "Epoch 0 / 40... step 3...\t training loss: 3.6272614002227783\n",
      "Epoch 0 / 40... step 4...\t training loss: 2.741917371749878\n",
      "Epoch 0 / 40... step 5...\t training loss: 2.9585046768188477\n",
      "Epoch 0 / 40... step 6...\t training loss: 2.7560641765594482\n",
      "Epoch 0 / 40... step 7...\t training loss: 2.686645746231079\n",
      "Epoch 0 / 40... step 8...\t training loss: 2.7063076496124268\n",
      "Epoch 0 / 40... step 9...\t training loss: 2.605485200881958\n",
      "Epoch 0 / 40... step 10...\t training loss: 2.3965840339660645\n",
      "Epoch 0 / 40... step 11...\t training loss: 2.3593287467956543\n",
      "Epoch 0 / 40... step 12...\t training loss: 2.2483716011047363\n",
      "Epoch 0 / 40... step 13...\t training loss: 2.3453991413116455\n",
      "Epoch 0 / 40... step 14...\t training loss: 2.552450656890869\n",
      "Epoch 0 / 40... step 15...\t training loss: 2.607212543487549\n",
      "Epoch 0 / 40... step 16...\t training loss: 2.3656346797943115\n",
      "Epoch 0 / 40... step 17...\t training loss: 2.6270275115966797\n",
      "Epoch 0 / 40... step 18...\t training loss: 2.56378436088562\n",
      "Epoch 0 / 40... step 19...\t training loss: 2.4522626399993896\n",
      "Epoch 0 / 40... step 20...\t training loss: 2.4727673530578613\n",
      "Epoch 0 / 40... step 21...\t training loss: 2.615380048751831\n",
      "Epoch 0 / 40... step 22...\t training loss: 2.4873642921447754\n",
      "Epoch 0 / 40... step 23...\t training loss: 2.4381556510925293\n",
      "Epoch 0 / 40... step 24...\t training loss: 2.5154757499694824\n",
      "Epoch 0 / 40... step 25...\t training loss: 2.6097664833068848\n",
      "Epoch 0 / 40... step 26...\t training loss: 2.380048990249634\n",
      "Epoch 0 / 40... step 27...\t training loss: 2.435593366622925\n",
      "Epoch 0 / 40... step 28...\t training loss: 2.4903268814086914\n",
      "Epoch 0 / 40... step 29...\t training loss: 2.323657274246216\n",
      "Epoch 0 / 40... step 30...\t training loss: 2.433868169784546\n",
      "Epoch 0 / 40... step 31...\t training loss: 2.532865524291992\n",
      "Epoch 0 / 40... step 32...\t training loss: 2.452878952026367\n",
      "Epoch 0 / 40... step 33...\t training loss: 2.4919979572296143\n",
      "Epoch 0 / 40... step 34...\t training loss: 2.525289535522461\n",
      "Epoch 0 / 40... step 35...\t training loss: 2.478902578353882\n",
      "Epoch 0 / 40... step 36...\t training loss: 2.391964912414551\n",
      "Epoch 0 / 40... step 37...\t training loss: 2.636408805847168\n",
      "Epoch 0 / 40... step 38...\t training loss: 2.401007890701294\n",
      "Epoch 0 / 40... step 39...\t training loss: 2.218536853790283\n",
      "Epoch 0 / 40... step 40...\t training loss: 2.261289358139038\n",
      "Epoch 0 / 40... step 41...\t training loss: 2.4503307342529297\n",
      "Epoch 0 / 40... step 42...\t training loss: 2.2227184772491455\n",
      "Epoch 0 / 40... step 43...\t training loss: 2.4288105964660645\n",
      "Epoch 0 / 40... step 44...\t training loss: 2.399785041809082\n",
      "Epoch 0 / 40... step 45...\t training loss: 2.861889362335205\n",
      "Epoch 0 / 40... step 46...\t training loss: 2.3032772541046143\n",
      "Epoch 0 / 40... step 47...\t training loss: 2.3366665840148926\n",
      "Epoch 0 / 40... step 48...\t training loss: 2.547761917114258\n",
      "Epoch 0 / 40... step 49...\t training loss: 2.4640209674835205\n",
      "Epoch 0 / 40... step 50...\t training loss: 2.3367068767547607\n",
      "Epoch 0 / 40... step 51...\t training loss: 2.425081729888916\n",
      "Epoch 0 / 40... step 52...\t training loss: 2.483077049255371\n",
      "Epoch 0 / 40... step 53...\t training loss: 2.628969430923462\n",
      "Epoch 0 / 40... step 54...\t training loss: 2.628328561782837\n",
      "Epoch 0 / 40... step 55...\t training loss: 2.4348857402801514\n",
      "Epoch 0 / 40... step 56...\t training loss: 2.422966241836548\n",
      "Epoch 0 / 40... step 57...\t training loss: 2.5111727714538574\n",
      "Epoch 0 / 40... step 58...\t training loss: 2.574993848800659\n",
      "Epoch 0 / 40... step 59...\t training loss: 2.5028696060180664\n",
      "Epoch 0 / 40... step 60...\t training loss: 2.1914966106414795\n",
      "Epoch 0 / 40... step 61...\t training loss: 2.5543644428253174\n",
      "Epoch 0 / 40... step 62...\t training loss: 2.4353907108306885\n",
      "Epoch 0 / 40... step 63...\t training loss: 2.450726270675659\n",
      "Epoch 0 / 40... step 64...\t training loss: 2.2745511531829834\n",
      "Epoch 0 / 40... step 65...\t training loss: 2.329019069671631\n",
      "Epoch 0 / 40... step 66...\t training loss: 2.4844748973846436\n",
      "Epoch 0 / 40... step 67...\t training loss: 2.398787498474121\n",
      "Epoch 0 / 40... step 68...\t training loss: 2.3492519855499268\n",
      "Epoch 0 / 40... step 69...\t training loss: 2.206857681274414\n",
      "Epoch 0 / 40... step 70...\t training loss: 2.422961950302124\n",
      "Epoch 0 / 40... step 71...\t training loss: 2.503335952758789\n",
      "Epoch 0 / 40... step 72...\t training loss: 2.3124959468841553\n",
      "Epoch 0 / 40... step 73...\t training loss: 2.3365962505340576\n",
      "Epoch 0 / 40... step 74...\t training loss: 2.5960593223571777\n",
      "Epoch 0 / 40... step 75...\t training loss: 2.393049716949463\n",
      "Epoch 0 / 40... step 76...\t training loss: 2.333312511444092\n",
      "Epoch 0 / 40... step 77...\t training loss: 2.4890594482421875\n",
      "Epoch 0 / 40... step 78...\t training loss: 2.717261791229248\n",
      "Epoch 0 / 40... step 79...\t training loss: 2.3458423614501953\n",
      "Epoch 0 / 40... step 80...\t training loss: 2.4009292125701904\n",
      "Epoch 0 / 40... step 81...\t training loss: 2.4063172340393066\n",
      "Epoch 0 / 40... step 82...\t training loss: 2.492333173751831\n",
      "Epoch 0 / 40... step 83...\t training loss: 2.417701244354248\n",
      "Epoch 0 / 40... step 84...\t training loss: 2.2884414196014404\n",
      "Epoch 0 / 40... step 85...\t training loss: 2.3948473930358887\n",
      "Epoch 0 / 40... step 86...\t training loss: 2.1660733222961426\n",
      "Epoch 0 / 40... step 87...\t training loss: 2.2584033012390137\n",
      "Epoch 0 / 40... step 88...\t training loss: 2.6685476303100586\n",
      "Epoch 0 / 40... step 89...\t training loss: 2.500497817993164\n",
      "Epoch 0 / 40... step 90...\t training loss: 2.553945541381836\n",
      "Epoch 0 / 40... step 91...\t training loss: 2.3659512996673584\n",
      "Epoch 0 / 40... step 92...\t training loss: 2.101435899734497\n",
      "Epoch 0 / 40... step 93...\t training loss: 2.4223928451538086\n",
      "Epoch 0 / 40... step 94...\t training loss: 2.349031448364258\n",
      "Epoch 0 / 40... step 95...\t training loss: 2.507190465927124\n",
      "Epoch 0 / 40... step 96...\t training loss: 2.4045562744140625\n",
      "Epoch 0 / 40... step 97...\t training loss: 2.4070990085601807\n",
      "Epoch 0 / 40... step 98...\t training loss: 2.3159263134002686\n",
      "Epoch 0 / 40... step 99...\t training loss: 2.4836387634277344\n",
      "Epoch 0 / 40... step 100...\t training loss: 2.4544484615325928\n",
      "Epoch 0 / 40... step 101...\t training loss: 2.4191510677337646\n",
      "Epoch 0 / 40... step 102...\t training loss: 2.3695621490478516\n",
      "Epoch 0 / 40... step 103...\t training loss: 2.6653058528900146\n",
      "Epoch 0 / 40... step 104...\t training loss: 2.3143887519836426\n",
      "Epoch 0 / 40... step 105...\t training loss: 2.5241739749908447\n",
      "Epoch 0 / 40... step 106...\t training loss: 2.5523183345794678\n",
      "Epoch 0 / 40... step 107...\t training loss: 2.4361977577209473\n",
      "Epoch 0 / 40... step 108...\t training loss: 2.2108607292175293\n",
      "Epoch 0 / 40... step 109...\t training loss: 2.577115774154663\n",
      "Epoch 0 / 40... step 110...\t training loss: 2.61244535446167\n",
      "Epoch 0 / 40... step 111...\t training loss: 2.297821044921875\n",
      "Epoch 0 / 40... step 112...\t training loss: 2.513347864151001\n",
      "Epoch 0 / 40... step 113...\t training loss: 2.1669533252716064\n",
      "Epoch 0 / 40... step 114...\t training loss: 2.7223410606384277\n",
      "Epoch 0 / 40... step 115...\t training loss: 2.5047805309295654\n",
      "Epoch 0 / 40... step 116...\t training loss: 2.4611093997955322\n",
      "Epoch 0 / 40... step 117...\t training loss: 2.4161603450775146\n",
      "Epoch 0 / 40... step 118...\t training loss: 2.460552453994751\n",
      "Epoch 0 / 40... step 119...\t training loss: 2.3812754154205322\n",
      "Epoch 0 / 40... step 120...\t training loss: 2.5943210124969482\n",
      "Epoch 0 / 40... step 121...\t training loss: 2.603710651397705\n",
      "Epoch 0 / 40... step 122...\t training loss: 2.3223257064819336\n",
      "Epoch 0 / 40... step 123...\t training loss: 2.693666696548462\n",
      "Epoch 0 / 40... step 124...\t training loss: 2.4459688663482666\n",
      "Epoch 0 / 40... step 125...\t training loss: 2.3488171100616455\n",
      "Epoch 0 / 40... step 126...\t training loss: 2.387795925140381\n",
      "Epoch 0 / 40... step 127...\t training loss: 2.4513206481933594\n",
      "Epoch 0 / 40... step 128...\t training loss: 2.339465856552124\n",
      "Epoch 0 / 40... step 129...\t training loss: 2.4131667613983154\n",
      "Epoch 0 / 40... step 130...\t training loss: 2.4167115688323975\n",
      "Epoch 0 / 40... step 131...\t training loss: 2.5457615852355957\n",
      "Epoch 0 / 40... step 132...\t training loss: 2.328683376312256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 40... step 0...\t training loss: 2.342819929122925\n",
      "Epoch 1 / 40... step 1...\t training loss: 2.4158496856689453\n",
      "Epoch 1 / 40... step 2...\t training loss: 2.5047764778137207\n",
      "Epoch 1 / 40... step 3...\t training loss: 2.6267623901367188\n",
      "Epoch 1 / 40... step 4...\t training loss: 2.39003586769104\n",
      "Epoch 1 / 40... step 5...\t training loss: 2.4722237586975098\n",
      "Epoch 1 / 40... step 6...\t training loss: 2.4351279735565186\n",
      "Epoch 1 / 40... step 7...\t training loss: 2.3943026065826416\n",
      "Epoch 1 / 40... step 8...\t training loss: 2.515941858291626\n",
      "Epoch 1 / 40... step 9...\t training loss: 2.3855631351470947\n",
      "Epoch 1 / 40... step 10...\t training loss: 2.3944480419158936\n",
      "Epoch 1 / 40... step 11...\t training loss: 2.3388748168945312\n",
      "Epoch 1 / 40... step 12...\t training loss: 2.034548759460449\n",
      "Epoch 1 / 40... step 13...\t training loss: 2.328373670578003\n",
      "Epoch 1 / 40... step 14...\t training loss: 2.650632381439209\n",
      "Epoch 1 / 40... step 15...\t training loss: 2.4293878078460693\n",
      "Epoch 1 / 40... step 16...\t training loss: 2.5272395610809326\n",
      "Epoch 1 / 40... step 17...\t training loss: 2.371270179748535\n",
      "Epoch 1 / 40... step 18...\t training loss: 2.4599945545196533\n",
      "Epoch 1 / 40... step 19...\t training loss: 2.539647340774536\n",
      "Epoch 1 / 40... step 20...\t training loss: 2.551154613494873\n",
      "Epoch 1 / 40... step 21...\t training loss: 2.1317334175109863\n",
      "Epoch 1 / 40... step 22...\t training loss: 2.388885498046875\n",
      "Epoch 1 / 40... step 23...\t training loss: 2.6364028453826904\n",
      "Epoch 1 / 40... step 24...\t training loss: 2.376368522644043\n",
      "Epoch 1 / 40... step 25...\t training loss: 2.414236307144165\n",
      "Epoch 1 / 40... step 26...\t training loss: 2.3212637901306152\n",
      "Epoch 1 / 40... step 27...\t training loss: 2.366274833679199\n",
      "Epoch 1 / 40... step 28...\t training loss: 2.534963607788086\n",
      "Epoch 1 / 40... step 29...\t training loss: 2.320998191833496\n",
      "Epoch 1 / 40... step 30...\t training loss: 2.3589096069335938\n",
      "Epoch 1 / 40... step 31...\t training loss: 2.5412139892578125\n",
      "Epoch 1 / 40... step 32...\t training loss: 2.459105968475342\n",
      "Epoch 1 / 40... step 33...\t training loss: 2.478015422821045\n",
      "Epoch 1 / 40... step 34...\t training loss: 2.503772497177124\n",
      "Epoch 1 / 40... step 35...\t training loss: 2.351492404937744\n",
      "Epoch 1 / 40... step 36...\t training loss: 2.414259910583496\n",
      "Epoch 1 / 40... step 37...\t training loss: 2.256692409515381\n",
      "Epoch 1 / 40... step 38...\t training loss: 2.4834113121032715\n",
      "Epoch 1 / 40... step 39...\t training loss: 2.5043253898620605\n",
      "Epoch 1 / 40... step 40...\t training loss: 2.309567928314209\n",
      "Epoch 1 / 40... step 41...\t training loss: 2.4935708045959473\n",
      "Epoch 1 / 40... step 42...\t training loss: 2.4904558658599854\n",
      "Epoch 1 / 40... step 43...\t training loss: 2.511822462081909\n",
      "Epoch 1 / 40... step 44...\t training loss: 2.2483971118927\n",
      "Epoch 1 / 40... step 45...\t training loss: 2.5806169509887695\n",
      "Epoch 1 / 40... step 46...\t training loss: 2.496224880218506\n",
      "Epoch 1 / 40... step 47...\t training loss: 2.335314989089966\n",
      "Epoch 1 / 40... step 48...\t training loss: 2.306041955947876\n",
      "Epoch 1 / 40... step 49...\t training loss: 2.5232441425323486\n",
      "Epoch 1 / 40... step 50...\t training loss: 2.4774117469787598\n",
      "Epoch 1 / 40... step 51...\t training loss: 2.3743088245391846\n",
      "Epoch 1 / 40... step 52...\t training loss: 2.630300998687744\n",
      "Epoch 1 / 40... step 53...\t training loss: 2.5110864639282227\n",
      "Epoch 1 / 40... step 54...\t training loss: 2.3006591796875\n",
      "Epoch 1 / 40... step 55...\t training loss: 2.505347967147827\n",
      "Epoch 1 / 40... step 56...\t training loss: 2.245973825454712\n",
      "Epoch 1 / 40... step 57...\t training loss: 2.4884231090545654\n",
      "Epoch 1 / 40... step 58...\t training loss: 2.400646209716797\n",
      "Epoch 1 / 40... step 59...\t training loss: 2.4727590084075928\n",
      "Epoch 1 / 40... step 60...\t training loss: 2.3170740604400635\n",
      "Epoch 1 / 40... step 61...\t training loss: 2.4701526165008545\n",
      "Epoch 1 / 40... step 62...\t training loss: 2.533987522125244\n",
      "Epoch 1 / 40... step 63...\t training loss: 2.1518216133117676\n",
      "Epoch 1 / 40... step 64...\t training loss: 2.3763511180877686\n",
      "Epoch 1 / 40... step 65...\t training loss: 2.495297431945801\n",
      "Epoch 1 / 40... step 66...\t training loss: 2.4837183952331543\n",
      "Epoch 1 / 40... step 67...\t training loss: 2.4617700576782227\n",
      "Epoch 1 / 40... step 68...\t training loss: 2.3472297191619873\n",
      "Epoch 1 / 40... step 69...\t training loss: 2.350644111633301\n",
      "Epoch 1 / 40... step 70...\t training loss: 2.5198636054992676\n",
      "Epoch 1 / 40... step 71...\t training loss: 2.370727062225342\n",
      "Epoch 1 / 40... step 72...\t training loss: 2.5091187953948975\n",
      "Epoch 1 / 40... step 73...\t training loss: 2.407266139984131\n",
      "Epoch 1 / 40... step 74...\t training loss: 2.60428524017334\n",
      "Epoch 1 / 40... step 75...\t training loss: 2.2818984985351562\n",
      "Epoch 1 / 40... step 76...\t training loss: 2.4279303550720215\n",
      "Epoch 1 / 40... step 77...\t training loss: 2.2202775478363037\n",
      "Epoch 1 / 40... step 78...\t training loss: 2.202852725982666\n",
      "Epoch 1 / 40... step 79...\t training loss: 2.412585735321045\n",
      "Epoch 1 / 40... step 80...\t training loss: 2.402735948562622\n",
      "Epoch 1 / 40... step 81...\t training loss: 2.211791515350342\n",
      "Epoch 1 / 40... step 82...\t training loss: 2.342108726501465\n",
      "Epoch 1 / 40... step 83...\t training loss: 2.4712421894073486\n",
      "Epoch 1 / 40... step 84...\t training loss: 2.3988842964172363\n",
      "Epoch 1 / 40... step 85...\t training loss: 2.4229888916015625\n",
      "Epoch 1 / 40... step 86...\t training loss: 2.2820217609405518\n",
      "Epoch 1 / 40... step 87...\t training loss: 2.487114906311035\n",
      "Epoch 1 / 40... step 88...\t training loss: 2.4065561294555664\n",
      "Epoch 1 / 40... step 89...\t training loss: 2.189129114151001\n",
      "Epoch 1 / 40... step 90...\t training loss: 2.2160770893096924\n",
      "Epoch 1 / 40... step 91...\t training loss: 2.4608898162841797\n",
      "Epoch 1 / 40... step 92...\t training loss: 1.8777471780776978\n",
      "Epoch 1 / 40... step 93...\t training loss: 2.252352714538574\n",
      "Epoch 1 / 40... step 94...\t training loss: 2.114830732345581\n",
      "Epoch 1 / 40... step 95...\t training loss: 2.122145414352417\n",
      "Epoch 1 / 40... step 96...\t training loss: 2.0368528366088867\n",
      "Epoch 1 / 40... step 97...\t training loss: 2.4477505683898926\n",
      "Epoch 1 / 40... step 98...\t training loss: 2.196605682373047\n",
      "Epoch 1 / 40... step 99...\t training loss: 2.46791672706604\n",
      "Epoch 1 / 40... step 100...\t training loss: 2.1102283000946045\n",
      "Epoch 1 / 40... step 101...\t training loss: 2.2267906665802\n",
      "Epoch 1 / 40... step 102...\t training loss: 2.172630548477173\n",
      "Epoch 1 / 40... step 103...\t training loss: 2.2858033180236816\n",
      "Epoch 1 / 40... step 104...\t training loss: 2.253572940826416\n",
      "Epoch 1 / 40... step 105...\t training loss: 2.326974630355835\n",
      "Epoch 1 / 40... step 106...\t training loss: 2.1333281993865967\n",
      "Epoch 1 / 40... step 107...\t training loss: 2.2099125385284424\n",
      "Epoch 1 / 40... step 108...\t training loss: 2.1986641883850098\n",
      "Epoch 1 / 40... step 109...\t training loss: 2.0870718955993652\n",
      "Epoch 1 / 40... step 110...\t training loss: 2.3881218433380127\n",
      "Epoch 1 / 40... step 111...\t training loss: 1.9439949989318848\n",
      "Epoch 1 / 40... step 112...\t training loss: 2.070979595184326\n",
      "Epoch 1 / 40... step 113...\t training loss: 2.042335033416748\n",
      "Epoch 1 / 40... step 114...\t training loss: 2.1376349925994873\n",
      "Epoch 1 / 40... step 115...\t training loss: 2.1928470134735107\n",
      "Epoch 1 / 40... step 116...\t training loss: 2.2051050662994385\n",
      "Epoch 1 / 40... step 117...\t training loss: 2.095484972000122\n",
      "Epoch 1 / 40... step 118...\t training loss: 2.036273241043091\n",
      "Epoch 1 / 40... step 119...\t training loss: 2.156019687652588\n",
      "Epoch 1 / 40... step 120...\t training loss: 2.0109047889709473\n",
      "Epoch 1 / 40... step 121...\t training loss: 2.2832789421081543\n",
      "Epoch 1 / 40... step 122...\t training loss: 2.1691579818725586\n",
      "Epoch 1 / 40... step 123...\t training loss: 2.274460554122925\n",
      "Epoch 1 / 40... step 124...\t training loss: 2.1745951175689697\n",
      "Epoch 1 / 40... step 125...\t training loss: 2.020794153213501\n",
      "Epoch 1 / 40... step 126...\t training loss: 2.159498453140259\n",
      "Epoch 1 / 40... step 127...\t training loss: 2.2532379627227783\n",
      "Epoch 1 / 40... step 128...\t training loss: 2.0825247764587402\n",
      "Epoch 1 / 40... step 129...\t training loss: 2.0278542041778564\n",
      "Epoch 1 / 40... step 130...\t training loss: 2.279772996902466\n",
      "Epoch 1 / 40... step 131...\t training loss: 2.196531295776367\n",
      "Epoch 1 / 40... step 132...\t training loss: 2.060297727584839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 40... step 0...\t training loss: 2.172239303588867\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, nr_epochs):\n",
    "    slope_value = min(max_slope, init_slope_value + slope_annealing_increase_per_epoch * epoch)\n",
    "    df_filtered = df_filtered.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    for i in range(0, dataset_size // batch_size):\n",
    "        batch_idx = i * batch_size\n",
    "        \n",
    "        start_idx = batch_idx\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        df_batch = df_filtered.iloc[start_idx:end_idx]\n",
    "        comment_ints_batch = df_batch[comment_int_colname].tolist()\n",
    "        comment_ints_batch = np.array(comment_ints_batch, dtype=np.int32)\n",
    "\n",
    "        xs_batch = comment_ints_batch[:,0:J]\n",
    "        ys_batch = comment_ints_batch[:,1:(J+1)]\n",
    "\n",
    "        feed_dict = {model.xs: xs_batch, \n",
    "                     model.ys: ys_batch, \n",
    "                     model.slope_annealing_placeholder: slope_value}\n",
    "\n",
    "        _, loss_batch = sess.run(\n",
    "            [model.train_op, model.loss], feed_dict=feed_dict\n",
    "        )\n",
    "        \n",
    "        print(\"Epoch {} / {}... step {}...\\t training loss: {}\".format(\n",
    "                epoch, nr_epochs, i, loss_batch))\n",
    "        \n",
    "    if epoch > 0 and (epoch % 2 == 0):\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir = 'checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "saver.save(sess, os.path.join(checkpoint_dir, \"hm_lstm_L{}_h{}_e{}.ckpt\".format(\n",
    "                    num_layers, hidden_dim, nr_epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, nr_chars, top_n=4):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(nr_chars, 1, p=p)[0]\n",
    "    return c\n",
    "\n",
    "CONTROL_CHARS = [go, end_of_text, pad, end_of_padded_comment]\n",
    "\n",
    "def sample(checkpoint, J, hidden_dim, V, prime=\"The \"):\n",
    "    \n",
    "    samples = [c for c in prime]\n",
    "    model = Model(batch_size, J, V, emb_dim, hidden_dim, num_layers, \n",
    "                  learning_rate, grad_clip, sampling=False)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        zero_state = sess.run(model.initial_state)\n",
    "        \n",
    "        prime = ''.join([go] + list(prime))\n",
    "        \n",
    "        x = np.zeros((batch_size, J))\n",
    "        \n",
    "        for i in range(0,len(prime)):\n",
    "            c = prime[i]\n",
    "            x[:,i] = np.array([char2int[c] for _ in range(0, batch_size)])\n",
    "            feed = {model.xs: x,\n",
    "                    model.slope_annealing_placeholder: 5.0,\n",
    "                    model.initial_state: zero_state}\n",
    "            preds_ch, new_state = sess.run(\n",
    "                [model.probabilities, model.state], \n",
    "                feed_dict=feed)\n",
    "            \n",
    "            preds_ch = np.reshape(preds_ch, [batch_size, J, V])[0, i, :]\n",
    "            char_id = pick_top_n(preds_ch, nr_chars=V, top_n=4)\n",
    "            \n",
    "        if int2char[char_id] in CONTROL_CHARS:\n",
    "            return ''.join(samples)\n",
    "        else:\n",
    "            samples.append(int2char[char_id])\n",
    "\n",
    "        for i in range(len(prime), J):\n",
    "            x[:,i] = np.array([char_id for _ in range(0, batch_size)])\n",
    "            feed = {model.xs: x,\n",
    "                    model.slope_annealing_placeholder: 5.0,\n",
    "                    model.initial_state: zero_state}\n",
    "            preds_ch, new_state = sess.run(\n",
    "                [model.probabilities, model.state], \n",
    "                feed_dict=feed)\n",
    "                \n",
    "            preds_ch = np.reshape(preds_ch, [batch_size, J, V])[0, i, :]\n",
    "            char_id = pick_top_n(preds_ch, nr_chars=V, top_n=2)\n",
    "            if int2char[char_id] in CONTROL_CHARS:\n",
    "                break\n",
    "            else:\n",
    "                samples.append(int2char[char_id])\n",
    "    \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/hm_lstm_L3_h300_e10.ckpt'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each h_layer output should have shape [batch_size, timesteps, hidden dim]\n",
      "[40, 100, 300]\n",
      "[40, 100, 300]\n",
      "[40, 100, 300]\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/hm_lstm_L3_h300_e10.ckpt\n"
     ]
    }
   ],
   "source": [
    "samp = sample(checkpoint, 100, hidden_dim, V, prime=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x',\n",
       " 'o',\n",
       " '!',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '!',\n",
       " '!',\n",
       " '.',\n",
       " '!',\n",
       " '.',\n",
       " '!',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '!',\n",
       " '.',\n",
       " '!',\n",
       " '!',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each h_layer output should have shape [batch_size, timesteps, hidden dim]\n",
      "[40, 100, 300]\n",
      "[40, 100, 300]\n",
      "[40, 100, 300]\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/hm_lstm_L3_h300_e10.ckpt\n"
     ]
    }
   ],
   "source": [
    "samp = sample(checkpoint, 100, hidden_dim, V, prime=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thee   e e e      e e ee  ee '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
