{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from HM_LSTM_Cell import HM_LSTM_Cell\n",
    "from Multi_HM_LSTM_Cell import Multi_HM_LSTM_Cell\n",
    "from utils import HM_LSTM_StateTuple\n",
    "import tensorflow as tf\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = 'checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### HYPERPARAMS #######\n",
    "batch_size = 64\n",
    "J = 100\n",
    "emb_dim = 128\n",
    "hidden_dim = 512\n",
    "num_layers = 3\n",
    "output_emb_dim = 512\n",
    "\n",
    "num_epochs = 125\n",
    "\n",
    "init_learning_rate = 0.002\n",
    "minimum_learning_rate = 0.0001\n",
    "learning_rate_annealing_const = 0.8\n",
    "learning_rate_epochs_per_annealing = 25\n",
    "grad_clip = 1.0\n",
    "\n",
    "init_slope_value = 1.0\n",
    "max_slope = 5.0\n",
    "slope_annealing_increase_per_epoch = 0.04  # slope = min(5.0, 1.0 + 0.04 * epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hosh(x):\n",
    "    hv = 0\n",
    "    for c in list(str(x)):\n",
    "        hv = hv ^ ord(c)\n",
    "    return hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### DATASET PREP #######\n",
    "config = configparser.ConfigParser()\n",
    "config.read('.config')\n",
    "\n",
    "fp = config['default']['fp']\n",
    "label_colname = config['default']['label_colname']\n",
    "text_colname = config['default']['text_colname']\n",
    "\n",
    "provider_column_names = [label_colname, text_colname]\n",
    "provider_label_values_hashed = [8, 70]\n",
    "\n",
    "df = pd.read_csv(fp, sep='\\t', names=provider_column_names, skiprows=1, quoting=csv.QUOTE_NONE, quotechar='|', escapechar='\\\\')\n",
    "df = df[df[label_colname].apply(lambda x: hosh(x) in provider_label_values_hashed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    return ''.join([c for c in x if ord(c) < 128])\n",
    "\n",
    "clean_text_colname = 'clean_' + text_colname\n",
    "df[clean_text_colname] = df[text_colname].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cond(x):\n",
    "    if len(x) + 3 <= J:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df_filtered = df.copy(deep=True)\n",
    "df_filtered = df_filtered[df_filtered[clean_text_colname].apply(filter_cond)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_counter = Counter()\n",
    "comments = df_filtered[clean_text_colname].tolist()\n",
    "\n",
    "for comment_text in comments:\n",
    "    comment_chars = list(comment_text)\n",
    "    char_counter.update(comment_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 47490),\n",
       " ('e', 22918),\n",
       " ('t', 16863),\n",
       " ('a', 15616),\n",
       " ('o', 15240),\n",
       " ('i', 14160),\n",
       " ('s', 13762),\n",
       " ('n', 13133),\n",
       " ('r', 10814),\n",
       " ('h', 10299),\n",
       " ('l', 9264),\n",
       " ('u', 6812),\n",
       " ('d', 6590),\n",
       " ('m', 5254),\n",
       " ('.', 4949),\n",
       " ('c', 4777),\n",
       " ('y', 4611),\n",
       " ('p', 4214),\n",
       " ('g', 4094),\n",
       " ('w', 4011),\n",
       " ('f', 3261),\n",
       " ('b', 3033),\n",
       " ('k', 2387),\n",
       " ('T', 2040),\n",
       " ('v', 1936),\n",
       " ('!', 1740),\n",
       " ('I', 1692),\n",
       " ('S', 1421),\n",
       " (\"'\", 1345),\n",
       " ('A', 1332),\n",
       " ('L', 1116),\n",
       " ('N', 1104),\n",
       " ('O', 1087),\n",
       " (',', 1085),\n",
       " ('E', 991),\n",
       " ('H', 958),\n",
       " ('C', 848),\n",
       " ('?', 809),\n",
       " ('R', 801),\n",
       " ('D', 799),\n",
       " ('M', 746),\n",
       " ('P', 744),\n",
       " ('W', 721),\n",
       " ('B', 492),\n",
       " ('G', 457),\n",
       " ('j', 446),\n",
       " ('F', 406),\n",
       " ('Y', 392),\n",
       " ('U', 364),\n",
       " ('\"', 328),\n",
       " ('K', 320),\n",
       " ('x', 267),\n",
       " ('0', 255),\n",
       " ('-', 214),\n",
       " ('z', 212),\n",
       " ('J', 203),\n",
       " ('2', 190),\n",
       " (':', 171),\n",
       " ('V', 135),\n",
       " ('1', 128),\n",
       " (')', 93),\n",
       " ('3', 76),\n",
       " ('4', 66),\n",
       " ('q', 64),\n",
       " ('#', 56),\n",
       " ('5', 56),\n",
       " ('(', 49),\n",
       " ('/', 45),\n",
       " ('7', 40),\n",
       " ('9', 37),\n",
       " ('6', 36),\n",
       " ('8', 36),\n",
       " ('=', 34),\n",
       " ('%', 31),\n",
       " ('X', 29),\n",
       " ('Q', 24),\n",
       " ('*', 21),\n",
       " ('&', 19),\n",
       " (';', 19),\n",
       " ('Z', 13),\n",
       " ('>', 11),\n",
       " ('~', 10),\n",
       " ('@', 9),\n",
       " ('$', 7),\n",
       " ('<', 6),\n",
       " ('^', 6),\n",
       " ('+', 3),\n",
       " ('_', 2),\n",
       " ('[', 1),\n",
       " ('`', 1),\n",
       " (']', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = [k for k, v in char_counter.items() if v >= 40]\n",
    "\n",
    "go = '\\x00'\n",
    "end_of_text = '\\x01'\n",
    "pad = '\\x02'\n",
    "end_of_padded_comment = '\\x03'\n",
    "unk = '\\x04'\n",
    "\n",
    "vocab.append(go)\n",
    "vocab.append(end_of_text)\n",
    "vocab.append(pad)\n",
    "vocab.append(end_of_padded_comment)\n",
    "vocab.append(unk)\n",
    "\n",
    "vocab = sorted(vocab, key=lambda c: ord(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int2char = {i: c for i, c in enumerate(vocab)}\n",
    "char2int = {c: i for i, c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\x00': 0,\n",
       " '\\x01': 1,\n",
       " '\\x02': 2,\n",
       " '\\x03': 3,\n",
       " '\\x04': 4,\n",
       " ' ': 5,\n",
       " '!': 6,\n",
       " '\"': 7,\n",
       " '#': 8,\n",
       " \"'\": 9,\n",
       " '(': 10,\n",
       " ')': 11,\n",
       " ',': 12,\n",
       " '-': 13,\n",
       " '.': 14,\n",
       " '/': 15,\n",
       " '0': 16,\n",
       " '1': 17,\n",
       " '2': 18,\n",
       " '3': 19,\n",
       " '4': 20,\n",
       " '5': 21,\n",
       " '7': 22,\n",
       " ':': 23,\n",
       " '?': 24,\n",
       " 'A': 25,\n",
       " 'B': 26,\n",
       " 'C': 27,\n",
       " 'D': 28,\n",
       " 'E': 29,\n",
       " 'F': 30,\n",
       " 'G': 31,\n",
       " 'H': 32,\n",
       " 'I': 33,\n",
       " 'J': 34,\n",
       " 'K': 35,\n",
       " 'L': 36,\n",
       " 'M': 37,\n",
       " 'N': 38,\n",
       " 'O': 39,\n",
       " 'P': 40,\n",
       " 'R': 41,\n",
       " 'S': 42,\n",
       " 'T': 43,\n",
       " 'U': 44,\n",
       " 'V': 45,\n",
       " 'W': 46,\n",
       " 'Y': 47,\n",
       " 'a': 48,\n",
       " 'b': 49,\n",
       " 'c': 50,\n",
       " 'd': 51,\n",
       " 'e': 52,\n",
       " 'f': 53,\n",
       " 'g': 54,\n",
       " 'h': 55,\n",
       " 'i': 56,\n",
       " 'j': 57,\n",
       " 'k': 58,\n",
       " 'l': 59,\n",
       " 'm': 60,\n",
       " 'n': 61,\n",
       " 'o': 62,\n",
       " 'p': 63,\n",
       " 'q': 64,\n",
       " 'r': 65,\n",
       " 's': 66,\n",
       " 't': 67,\n",
       " 'u': 68,\n",
       " 'v': 69,\n",
       " 'w': 70,\n",
       " 'x': 71,\n",
       " 'y': 72,\n",
       " 'z': 73}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    token_list = []\n",
    "    token_list.append(go)\n",
    "    token_list.extend(list(x))\n",
    "    token_list.append(end_of_text)\n",
    "    token_list.extend([pad for _ in range(0, max(0, J-len(x)-2))])\n",
    "    token_list.append(end_of_padded_comment)\n",
    "    return ''.join(token_list)[0:(J+1)]\n",
    "\n",
    "standardized_text_colname = 'standardized_' + text_colname\n",
    "df_filtered[standardized_text_colname] = df_filtered[clean_text_colname].apply(standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token2int(x):\n",
    "    return [(char2int[c] if c in char2int else char2int[unk]) for c in x]\n",
    "\n",
    "comment_int_colname = 'comment_ints'\n",
    "df_filtered[comment_int_colname] = df_filtered[standardized_text_colname].apply(token2int)\n",
    "\n",
    "nr_filtered_provider_records = df_filtered.shape[0]\n",
    "dataset_size = batch_size * (nr_filtered_provider_records // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = len(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "\n",
      "input chars:\n",
      "[ 0 32 56 59 59 48 65 72  5 27 59 56 61 67 62 61  9 66  5 52 60 48 56 59\n",
      " 66  5 63 65 62 69 52  5 66 55 52  5 56 66  5 48  5 50 62 65 65 68 63 67\n",
      "  5 54 59 62 49 48 59 56 66 67  5 65 56 66 62 67 67 62 13 52 48 67 52 65\n",
      "  6  6  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2]\n",
      "\n",
      "predict chars:\n",
      "[32 56 59 59 48 65 72  5 27 59 56 61 67 62 61  9 66  5 52 60 48 56 59 66\n",
      "  5 63 65 62 69 52  5 66 55 52  5 56 66  5 48  5 50 62 65 65 68 63 67  5\n",
      " 54 59 62 49 48 59 56 66 67  5 65 56 66 62 67 67 62 13 52 48 67 52 65  6\n",
      "  6  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  3]\n"
     ]
    }
   ],
   "source": [
    "####### Dataset format - inspect #######\n",
    "comment_ints_batch = df_filtered[comment_int_colname].iloc[0:batch_size].tolist()\n",
    "comment_ints_batch = np.array(comment_ints_batch, dtype=np.int32)\n",
    "xs_batch = comment_ints_batch[:,0:J]\n",
    "ys_batch = comment_ints_batch[:,1:(J+1)]\n",
    "print(\"Example:\\n\")\n",
    "print(\"input chars:\")\n",
    "print(xs_batch[0,:])\n",
    "print(\"\\npredict chars:\")\n",
    "print(ys_batch[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### HM-LSTM #######\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, batch_size, J, V, emb_dim, hidden_dim, output_emb_dim, num_layers, \n",
    "                 grad_clip):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        #if sampling == True:\n",
    "        #    batch_size, J = 1, 1\n",
    "        #else:\n",
    "        #    batch_size, J = batch_size, J\n",
    "        \n",
    "        self.V = V\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_emb_dim = output_emb_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.learning_rate = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "        self.slope_annealing_placeholder = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "        self.xs = tf.placeholder(tf.int32, [batch_size, J])\n",
    "        self.ys = tf.placeholder(tf.int32, [batch_size, J])\n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        \n",
    "        self.emb_initializer = tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "        self.emb_mat = tf.get_variable(\n",
    "            name='emb_mat', dtype=tf.float32, shape=[self.V, self.emb_dim],\n",
    "            initializer=self.emb_initializer\n",
    "        )\n",
    "        self.emb_xs = tf.nn.embedding_lookup(self.emb_mat, self.xs)\n",
    "\n",
    "        self.multi_cell = Multi_HM_LSTM_Cell([\n",
    "            HM_LSTM_Cell(\n",
    "                num_units=self.hidden_dim, \n",
    "                slope_annealing_placeholder=self.slope_annealing_placeholder,\n",
    "                forget_bias=1.0)\n",
    "            for _ in range(0, self.num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.zero_state = self.multi_cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # during deployment, our initial state should change as we compose something \n",
    "        # one token at a time. hence, we use a state_placeholder.\n",
    "        #\n",
    "        # since we can't pass state tuples around (booooo) we have package it as an array \n",
    "        # and then turn it back into a statetuple before feeding it to dynamic_rnn\n",
    "        #\n",
    "        # during training, we still wish to use zero state as the initial state.\n",
    "        # i tried to use a tf.where statement to do this, but tensorflow is stupid and \n",
    "        # it tries to package the output of tf.where as a tensor, and cannot handle tuples.\n",
    "        #\n",
    "        # as a workaround, we will get it via sess.run(model.zero_state) \n",
    "        # and then feed it to the state placeholder. \n",
    "        \n",
    "        self.state_placeholder = tf.placeholder(\n",
    "            tf.float32, [num_layers, batch_size, 2 * self.hidden_dim + 1])\n",
    "\n",
    "        # placeholder-derived tensors, tuples, etc.\n",
    "        layer_states = tf.unstack(self.state_placeholder, axis=0)\n",
    "        tuple_of_layer_states = tuple([\n",
    "            HM_LSTM_StateTuple(\n",
    "                c=layer_states[idx][:, 0:self.hidden_dim], \n",
    "                h=layer_states[idx][:, self.hidden_dim:(2*self.hidden_dim)], \n",
    "                z=layer_states[idx][:, (2*self.hidden_dim):]\n",
    "            )\n",
    "            for idx in range(0, num_layers)\n",
    "        ])\n",
    "\n",
    "        self.outputs, self.state = tf.nn.dynamic_rnn(\n",
    "            cell=self.multi_cell, inputs=self.emb_xs, initial_state=tuple_of_layer_states)\n",
    "\n",
    "        h_layer1, h_layer2, h_layer3 = self.outputs\n",
    "\n",
    "        h_layer1_per_char = tf.reshape(h_layer1, [-1, self.hidden_dim])\n",
    "        h_layer2_per_char = tf.reshape(h_layer2, [-1, self.hidden_dim])\n",
    "        h_layer3_per_char = tf.reshape(h_layer3, [-1, self.hidden_dim])\n",
    "\n",
    "        h_out_per_char = tf.concat(\n",
    "            [h_layer1_per_char, h_layer2_per_char, h_layer3_per_char], 1)\n",
    "        \n",
    "        g1 = tf.layers.dense(h_out_per_char, units=1, use_bias=False, activation=tf.nn.sigmoid)\n",
    "        g2 = tf.layers.dense(h_out_per_char, units=1, use_bias=False, activation=tf.nn.sigmoid)\n",
    "        g3 = tf.layers.dense(h_out_per_char, units=1, use_bias=False, activation=tf.nn.sigmoid)\n",
    "\n",
    "        self.output_emb = tf.layers.dense(\n",
    "            tf.concat([\n",
    "                g1 * h_layer1_per_char, \n",
    "                g2 * h_layer2_per_char, \n",
    "                g3 * h_layer3_per_char\n",
    "            ], 1), \n",
    "            units=self.output_emb_dim, \n",
    "            use_bias=False,\n",
    "            activation=None\n",
    "        )\n",
    "        self.output_emb = tf.maximum(0.10 * self.output_emb, self.output_emb)\n",
    "        \n",
    "        with tf.variable_scope('logit_layer'):\n",
    "            self.logits_kernel = tf.get_variable(name='logits_kernel', \n",
    "                shape=[self.output_emb_dim, self.V])\n",
    "    \n",
    "        self.logits = tf.matmul(self.output_emb, self.logits_kernel)\n",
    "        self.probabilities = tf.nn.softmax(self.logits)\n",
    "\n",
    "        self.loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels=self.ys,\n",
    "            logits=tf.reshape(self.logits, self.ys.get_shape().as_list() + [V])\n",
    "        )\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        gradients, _ = zip(*optimizer.compute_gradients(self.loss, tvars))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, grad_clip)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### RUN EXPERIMENT #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_uuid = str(uuid.uuid4())\n",
    "trial_dir = os.path.join(checkpoint_dir, trial_uuid)\n",
    "os.mkdir(trial_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/b6e2019e-bb61-4468-9d9f-a2b94529eadd\n"
     ]
    }
   ],
   "source": [
    "print(trial_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(batch_size, J, V, emb_dim, hidden_dim, output_emb_dim, num_layers, grad_clip)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_state = sess.run(model.zero_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_multilayer_statetuple_to_array(multilayer_statetuple):\n",
    "    current_state_array = np.stack([\n",
    "        np.concatenate([layer_state.c, layer_state.h, layer_state.z], -1) \n",
    "        for layer_state in multilayer_statetuple\n",
    "    ])\n",
    "    return current_state_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero_state_array = convert_multilayer_statetuple_to_array(zero_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 64, 1025)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_state_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 / 125... step 0...\t training loss: 0.556446373462677\n",
      "Epoch 17 / 125... step 1...\t training loss: 0.5849212408065796\n",
      "Epoch 17 / 125... step 2...\t training loss: 0.5369722843170166\n",
      "Epoch 17 / 125... step 3...\t training loss: 0.5312057137489319\n",
      "Epoch 17 / 125... step 4...\t training loss: 0.5473288893699646\n",
      "Epoch 17 / 125... step 5...\t training loss: 0.47616544365882874\n",
      "Epoch 17 / 125... step 6...\t training loss: 0.582590639591217\n",
      "Epoch 17 / 125... step 7...\t training loss: 0.5158740878105164\n",
      "Epoch 17 / 125... step 8...\t training loss: 0.46368148922920227\n",
      "Epoch 17 / 125... step 9...\t training loss: 0.49126532673835754\n",
      "Epoch 17 / 125... step 10...\t training loss: 0.5119099020957947\n",
      "Epoch 17 / 125... step 11...\t training loss: 0.5184198617935181\n",
      "Epoch 17 / 125... step 12...\t training loss: 0.4928800165653229\n",
      "Epoch 17 / 125... step 13...\t training loss: 0.5215612649917603\n",
      "Epoch 17 / 125... step 14...\t training loss: 0.48292914032936096\n",
      "Epoch 17 / 125... step 15...\t training loss: 0.5055919289588928\n",
      "Epoch 17 / 125... step 16...\t training loss: 0.4696691334247589\n",
      "Epoch 17 / 125... step 17...\t training loss: 0.5451894998550415\n",
      "Epoch 17 / 125... step 18...\t training loss: 0.5260657668113708\n",
      "Epoch 17 / 125... step 19...\t training loss: 0.5191042423248291\n",
      "Epoch 17 / 125... step 20...\t training loss: 0.5328900218009949\n",
      "Epoch 17 / 125... step 21...\t training loss: 0.5789011716842651\n",
      "Epoch 17 / 125... step 22...\t training loss: 0.4808295965194702\n",
      "Epoch 17 / 125... step 23...\t training loss: 0.5006807446479797\n",
      "Epoch 17 / 125... step 24...\t training loss: 0.5314851403236389\n",
      "Epoch 17 / 125... step 25...\t training loss: 0.5634833574295044\n",
      "Epoch 17 / 125... step 26...\t training loss: 0.4908696115016937\n",
      "Epoch 17 / 125... step 27...\t training loss: 0.4790537655353546\n",
      "Epoch 17 / 125... step 28...\t training loss: 0.5159378051757812\n",
      "Epoch 17 / 125... step 29...\t training loss: 0.5349220633506775\n",
      "Epoch 17 / 125... step 30...\t training loss: 0.5023130774497986\n",
      "Epoch 17 / 125... step 31...\t training loss: 0.6063066720962524\n",
      "Epoch 17 / 125... step 32...\t training loss: 0.5459893941879272\n",
      "Epoch 17 / 125... step 33...\t training loss: 0.5751063227653503\n",
      "Epoch 17 / 125... step 34...\t training loss: 0.5372175574302673\n",
      "Epoch 17 / 125... step 35...\t training loss: 0.5118014216423035\n",
      "Epoch 17 / 125... step 36...\t training loss: 0.537869393825531\n",
      "Epoch 17 / 125... step 37...\t training loss: 0.5031722784042358\n",
      "Epoch 17 / 125... step 38...\t training loss: 0.5598312616348267\n",
      "Epoch 17 / 125... step 39...\t training loss: 0.5410645008087158\n",
      "Epoch 17 / 125... step 40...\t training loss: 0.5249064564704895\n",
      "Epoch 17 / 125... step 41...\t training loss: 0.4994555115699768\n",
      "Epoch 17 / 125... step 42...\t training loss: 0.5108264684677124\n",
      "Epoch 17 / 125... step 43...\t training loss: 0.5489500164985657\n"
     ]
    }
   ],
   "source": [
    "learning_rate = init_learning_rate\n",
    "slope_value = init_slope_value\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    \n",
    "    if epoch > 0 and (epoch % learning_rate_epochs_per_annealing == 0):\n",
    "        learning_rate = max(\n",
    "            learning_rate_annealing_const * learning_rate, minimum_learning_rate)\n",
    "    \n",
    "    if epoch > 0:\n",
    "        slope_value = min(\n",
    "            slope_value + slope_annealing_increase_per_epoch, max_slope)\n",
    "        \n",
    "    df_filtered = df_filtered.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    for i in range(0, dataset_size // batch_size):\n",
    "        batch_idx = i * batch_size\n",
    "        \n",
    "        start_idx = batch_idx\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        df_batch = df_filtered.iloc[start_idx:end_idx]\n",
    "        comment_ints_batch = df_batch[comment_int_colname].tolist()\n",
    "        comment_ints_batch = np.array(comment_ints_batch, dtype=np.int32)\n",
    "\n",
    "        xs_batch = comment_ints_batch[:,0:J]\n",
    "        ys_batch = comment_ints_batch[:,1:(J+1)]\n",
    "\n",
    "        feed_dict = {model.xs: xs_batch, \n",
    "                     model.ys: ys_batch, \n",
    "                     model.slope_annealing_placeholder: slope_value, \n",
    "                     model.learning_rate: learning_rate, \n",
    "                     model.state_placeholder: zero_state_array\n",
    "                    }\n",
    "\n",
    "        _, loss_batch = sess.run(\n",
    "            [model.train_op, model.loss], \n",
    "            feed_dict=feed_dict\n",
    "        )\n",
    "        \n",
    "        print(\"Epoch {} / {}... step {}...\\t training loss: {}\".format(\n",
    "                epoch, num_epochs, i, loss_batch))\n",
    "\n",
    "    if epoch > 0 and (epoch % 2 == 0):\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        model_fp = os.path.join(\n",
    "            trial_dir, \"hm_lstm_L{}_e{}_h{}_o{}_epoch{}.ckpt\".format(\n",
    "                num_layers, emb_dim, hidden_dim, output_emb_dim, epoch))\n",
    "\n",
    "        saver.save(sess, model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/62f2c40d-b4aa-43d9-8eb6-83feccdb8b79/hm_lstm_L3_e128_h512_o512_epoch0.ckpt'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e = num_epochs\n",
    "e = 0\n",
    "\n",
    "saver.save(\n",
    "    sess, \n",
    "    os.path.join(trial_dir, \"hm_lstm_L{}_e{}_h{}_o{}_epoch{}.ckpt\".format(\n",
    "        num_layers, emb_dim, hidden_dim, output_emb_dim, e))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, nr_chars, top_n=4):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(nr_chars, 1, p=p)[0]\n",
    "    return c\n",
    "\n",
    "CONTROL_CHARS = [go, end_of_text, pad, end_of_padded_comment]\n",
    "\n",
    "def sample(checkpoint, max_sample_len, hidden_dim, V, prime=\"The \"):\n",
    "    \n",
    "    model = Model(1, 1, V, emb_dim, hidden_dim, output_emb_dim, num_layers, \n",
    "                  grad_clip)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        zero_state = sess.run(model.zero_state)\n",
    "        current_state = convert_multilayer_statetuple_to_array(zero_state)\n",
    "        \n",
    "        prime = ''.join([go] + list(clean(prime)))\n",
    "        samples = []\n",
    "        samples.extend([c for c in prime])\n",
    "        \n",
    "        x = np.zeros((sample_batch_size, 1))\n",
    "        \n",
    "        for i in range(0,len(prime)):\n",
    "            c = prime[i]\n",
    "            x[0,0] = char2int[c]\n",
    "            \n",
    "            feed = {model.xs: x,\n",
    "                    model.slope_annealing_placeholder: 5.0,\n",
    "                    model.state_placeholder: current_state}\n",
    "            preds_ch, new_state_tuple = sess.run(\n",
    "                [model.probabilities, model.state], \n",
    "                feed_dict=feed)\n",
    "            \n",
    "            char_id = pick_top_n(preds_ch, nr_chars=V, top_n=4)\n",
    "            current_state = convert_multilayer_statetuple_to_array(new_state_tuple)\n",
    "            \n",
    "        if int2char[char_id] in CONTROL_CHARS:\n",
    "            return ''.join(samples)\n",
    "        else:\n",
    "            samples.append(int2char[char_id])\n",
    "\n",
    "        for i in range(len(prime), max_sample_len):\n",
    "            x[0,0] = char_id\n",
    "            \n",
    "            feed = {model.xs: x,\n",
    "                    model.slope_annealing_placeholder: 5.0,\n",
    "                    model.state_placeholder: current_state}\n",
    "            preds_ch, new_state_tuple = sess.run(\n",
    "                [model.probabilities, model.state], \n",
    "                feed_dict=feed)\n",
    "                \n",
    "            char_id = pick_top_n(preds_ch, nr_chars=V, top_n=2)\n",
    "            current_state = convert_multilayer_statetuple_to_array(new_state_tuple)\n",
    "            \n",
    "            if int2char[char_id] in CONTROL_CHARS:\n",
    "                break\n",
    "            else:\n",
    "                samples.append(int2char[char_id])\n",
    "    \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint(trial_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/62f2c40d-b4aa-43d9-8eb6-83feccdb8b79/hm_lstm_L3_e128_h512_o512_epoch0.ckpt'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/62f2c40d-b4aa-43d9-8eb6-83feccdb8b79/hm_lstm_L3_e128_h512_o512_epoch0.ckpt\n"
     ]
    }
   ],
   "source": [
    "samp = sample(checkpoint, 100, hidden_dim, V, prime=\"Ok dude... \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00ok dude... ihe the ae te aee te tee toe ae toe te teee atee toe te tee te to at aee ate te tee te ae'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/62f2c40d-b4aa-43d9-8eb6-83feccdb8b79/hm_lstm_L3_e128_h512_o512_epoch0.ckpt\n"
     ]
    }
   ],
   "source": [
    "samp = sample(checkpoint, 100, hidden_dim, V, prime=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00oee tee te ate to ate ate ate at te aee ae tee at tat at ateete at te te ae ate tee to at ate te toe'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
