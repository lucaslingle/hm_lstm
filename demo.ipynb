{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from HM_LSTM_Cell import HM_LSTM_Cell\n",
    "from Multi_HM_LSTM_Cell import Multi_HM_LSTM_Cell\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from IPython.display import clear_output\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### HYPERPARAMS #######\n",
    "batch_size = 64\n",
    "J = 100\n",
    "emb_dim = 128\n",
    "hidden_dim = 512\n",
    "num_layers = 3\n",
    "output_emb_dim = 512\n",
    "\n",
    "num_epochs = 125\n",
    "\n",
    "init_learning_rate = 0.002\n",
    "minimum_learning_rate = 0.0001\n",
    "learning_rate_annealing_const = 0.8\n",
    "learning_rate_epochs_per_annealing = 25\n",
    "grad_clip = 1.0\n",
    "\n",
    "init_slope_value = 1.0\n",
    "max_slope = 5.0\n",
    "slope_annealing_increase_per_epoch = 0.04  # slope = min(5.0, 1.0 + 0.04 * epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hosh(x):\n",
    "    hv = 0\n",
    "    for c in list(str(x)):\n",
    "        hv = hv ^ ord(c)\n",
    "    return hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### DATASET PREP #######\n",
    "config = configparser.ConfigParser()\n",
    "config.read('.config')\n",
    "\n",
    "fp = config['default']['fp']\n",
    "label_colname = config['default']['label_colname']\n",
    "text_colname = config['default']['text_colname']\n",
    "\n",
    "provider_column_names = [label_colname, text_colname]\n",
    "provider_label_values_hashed = [8, 70]\n",
    "\n",
    "df = pd.read_csv(fp, sep='\\t', names=provider_column_names, skiprows=1, quoting=csv.QUOTE_NONE, quotechar='|', escapechar='\\\\')\n",
    "df = df[df[label_colname].apply(lambda x: hosh(x) in provider_label_values_hashed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    return ''.join([c.lower() for c in x if ord(c) < 128])\n",
    "\n",
    "clean_text_colname = 'clean_' + text_colname\n",
    "df[clean_text_colname] = df[text_colname].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cond(x):\n",
    "    if len(x) + 3 <= J:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df_filtered = df.copy(deep=True)\n",
    "df_filtered = df_filtered[df_filtered[clean_text_colname].apply(filter_cond)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_counter = Counter()\n",
    "comments = df_filtered[clean_text_colname].tolist()\n",
    "\n",
    "for comment_text in comments:\n",
    "    comment_chars = list(comment_text)\n",
    "    char_counter.update(comment_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 47490),\n",
       " ('e', 23909),\n",
       " ('t', 18903),\n",
       " ('a', 16948),\n",
       " ('o', 16327),\n",
       " ('i', 15852),\n",
       " ('s', 15183),\n",
       " ('n', 14237),\n",
       " ('r', 11615),\n",
       " ('h', 11257),\n",
       " ('l', 10380),\n",
       " ('d', 7389),\n",
       " ('u', 7176),\n",
       " ('m', 6000),\n",
       " ('c', 5625),\n",
       " ('y', 5003),\n",
       " ('p', 4958),\n",
       " ('.', 4949),\n",
       " ('w', 4732),\n",
       " ('g', 4551),\n",
       " ('f', 3667),\n",
       " ('b', 3525),\n",
       " ('k', 2707),\n",
       " ('v', 2071),\n",
       " ('!', 1740),\n",
       " (\"'\", 1345),\n",
       " (',', 1085),\n",
       " ('?', 809),\n",
       " ('j', 649),\n",
       " ('\"', 328),\n",
       " ('x', 296),\n",
       " ('0', 255),\n",
       " ('z', 225),\n",
       " ('-', 214),\n",
       " ('2', 190),\n",
       " (':', 171),\n",
       " ('1', 128),\n",
       " (')', 93),\n",
       " ('q', 88),\n",
       " ('3', 76),\n",
       " ('4', 66),\n",
       " ('#', 56),\n",
       " ('5', 56),\n",
       " ('(', 49),\n",
       " ('/', 45),\n",
       " ('7', 40),\n",
       " ('9', 37),\n",
       " ('6', 36),\n",
       " ('8', 36),\n",
       " ('=', 34),\n",
       " ('%', 31),\n",
       " ('*', 21),\n",
       " ('&', 19),\n",
       " (';', 19),\n",
       " ('>', 11),\n",
       " ('~', 10),\n",
       " ('@', 9),\n",
       " ('$', 7),\n",
       " ('<', 6),\n",
       " ('^', 6),\n",
       " ('+', 3),\n",
       " ('_', 2),\n",
       " (']', 1),\n",
       " ('`', 1),\n",
       " ('[', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = [k for k, v in char_counter.items() if v > 90]\n",
    "\n",
    "go = '\\x00'\n",
    "end_of_text = '\\x01'\n",
    "pad = '\\x02'\n",
    "end_of_padded_comment = '\\x03'\n",
    "unk = '\\x04'\n",
    "\n",
    "vocab.append(go)\n",
    "vocab.append(end_of_text)\n",
    "vocab.append(pad)\n",
    "vocab.append(end_of_padded_comment)\n",
    "vocab.append(unk)\n",
    "\n",
    "vocab = sorted(vocab, key=lambda c: ord(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int2char = {i: c for i, c in enumerate(vocab)}\n",
    "char2int = {c: i for i, c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\x00',\n",
       " 1: '\\x01',\n",
       " 2: '\\x02',\n",
       " 3: '\\x03',\n",
       " 4: '\\x04',\n",
       " 5: ' ',\n",
       " 6: '!',\n",
       " 7: '\"',\n",
       " 8: \"'\",\n",
       " 9: ')',\n",
       " 10: ',',\n",
       " 11: '-',\n",
       " 12: '.',\n",
       " 13: '0',\n",
       " 14: '1',\n",
       " 15: '2',\n",
       " 16: ':',\n",
       " 17: '?',\n",
       " 18: 'a',\n",
       " 19: 'b',\n",
       " 20: 'c',\n",
       " 21: 'd',\n",
       " 22: 'e',\n",
       " 23: 'f',\n",
       " 24: 'g',\n",
       " 25: 'h',\n",
       " 26: 'i',\n",
       " 27: 'j',\n",
       " 28: 'k',\n",
       " 29: 'l',\n",
       " 30: 'm',\n",
       " 31: 'n',\n",
       " 32: 'o',\n",
       " 33: 'p',\n",
       " 34: 'r',\n",
       " 35: 's',\n",
       " 36: 't',\n",
       " 37: 'u',\n",
       " 38: 'v',\n",
       " 39: 'w',\n",
       " 40: 'x',\n",
       " 41: 'y',\n",
       " 42: 'z'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\x00': 0,\n",
       " '\\x01': 1,\n",
       " '\\x02': 2,\n",
       " '\\x03': 3,\n",
       " '\\x04': 4,\n",
       " ' ': 5,\n",
       " '!': 6,\n",
       " '\"': 7,\n",
       " \"'\": 8,\n",
       " ')': 9,\n",
       " ',': 10,\n",
       " '-': 11,\n",
       " '.': 12,\n",
       " '0': 13,\n",
       " '1': 14,\n",
       " '2': 15,\n",
       " ':': 16,\n",
       " '?': 17,\n",
       " 'a': 18,\n",
       " 'b': 19,\n",
       " 'c': 20,\n",
       " 'd': 21,\n",
       " 'e': 22,\n",
       " 'f': 23,\n",
       " 'g': 24,\n",
       " 'h': 25,\n",
       " 'i': 26,\n",
       " 'j': 27,\n",
       " 'k': 28,\n",
       " 'l': 29,\n",
       " 'm': 30,\n",
       " 'n': 31,\n",
       " 'o': 32,\n",
       " 'p': 33,\n",
       " 'r': 34,\n",
       " 's': 35,\n",
       " 't': 36,\n",
       " 'u': 37,\n",
       " 'v': 38,\n",
       " 'w': 39,\n",
       " 'x': 40,\n",
       " 'y': 41,\n",
       " 'z': 42}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    token_list = []\n",
    "    token_list.append(go)\n",
    "    token_list.extend(list(x))\n",
    "    token_list.append(end_of_text)\n",
    "    token_list.extend([pad for _ in range(0, max(0, J-len(x)-2))])\n",
    "    token_list.append(end_of_padded_comment)\n",
    "    return ''.join(token_list)[0:(J+1)]\n",
    "\n",
    "standardized_text_colname = 'standardized_' + text_colname\n",
    "df_filtered[standardized_text_colname] = df_filtered[clean_text_colname].apply(standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token2int(x):\n",
    "    return [(char2int[c] if c in char2int else char2int[unk]) for c in x]\n",
    "\n",
    "comment_int_colname = 'comment_ints'\n",
    "df_filtered[comment_int_colname] = df_filtered[standardized_text_colname].apply(token2int)\n",
    "\n",
    "nr_filtered_provider_records = df_filtered.shape[0]\n",
    "dataset_size = batch_size * (nr_filtered_provider_records // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = len(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "\n",
      "input chars:\n",
      "[ 0 25 26 29 29 18 34 41  5 20 29 26 31 36 32 31  8 35  5 22 30 18 26 29\n",
      " 35  5 33 34 32 38 22  5 35 25 22  5 26 35  5 18  5 20 32 34 34 37 33 36\n",
      "  5 24 29 32 19 18 29 26 35 36  5 34 26 35 32 36 36 32 11 22 18 36 22 34\n",
      "  6  6  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2]\n",
      "\n",
      "predict chars:\n",
      "[25 26 29 29 18 34 41  5 20 29 26 31 36 32 31  8 35  5 22 30 18 26 29 35\n",
      "  5 33 34 32 38 22  5 35 25 22  5 26 35  5 18  5 20 32 34 34 37 33 36  5\n",
      " 24 29 32 19 18 29 26 35 36  5 34 26 35 32 36 36 32 11 22 18 36 22 34  6\n",
      "  6  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  3]\n"
     ]
    }
   ],
   "source": [
    "####### Dataset format - inspect #######\n",
    "comment_ints_batch = df_filtered[comment_int_colname].iloc[0:batch_size].tolist()\n",
    "comment_ints_batch = np.array(comment_ints_batch, dtype=np.int32)\n",
    "xs_batch = comment_ints_batch[:,0:J]\n",
    "ys_batch = comment_ints_batch[:,1:(J+1)]\n",
    "print(\"Example:\\n\")\n",
    "print(\"input chars:\")\n",
    "print(xs_batch[0,:])\n",
    "print(\"\\npredict chars:\")\n",
    "print(ys_batch[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [0, 25, 26, 29, 29, 18, 34, 41, 5, 20, 29, 26,...\n",
       "1     [0, 25, 26, 29, 29, 18, 34, 41, 5, 26, 35, 5, ...\n",
       "2     [0, 24, 32, 5, 19, 22, 34, 31, 26, 22, 10, 5, ...\n",
       "3     [0, 31, 32, 5, 30, 32, 34, 22, 5, 35, 18, 23, ...\n",
       "4     [0, 25, 26, 29, 29, 18, 34, 41, 5, 20, 29, 26,...\n",
       "5     [0, 19, 22, 34, 31, 26, 22, 5, 35, 18, 31, 21,...\n",
       "6     [0, 41, 32, 37, 5, 29, 26, 19, 36, 18, 34, 21,...\n",
       "7     [0, 26, 36, 8, 35, 5, 18, 29, 29, 5, 33, 18, 3...\n",
       "8     [0, 30, 18, 28, 22, 5, 32, 37, 34, 5, 41, 32, ...\n",
       "9     [0, 35, 22, 34, 26, 32, 37, 35, 29, 41, 10, 5,...\n",
       "10    [0, 39, 25, 18, 36, 5, 25, 18, 33, 33, 22, 31,...\n",
       "11    [0, 25, 32, 39, 5, 21, 32, 5, 18, 31, 41, 5, 3...\n",
       "12    [0, 26, 8, 30, 5, 35, 22, 34, 26, 32, 37, 35, ...\n",
       "13    [0, 36, 34, 37, 30, 33, 5, 39, 26, 29, 29, 5, ...\n",
       "14    [0, 36, 34, 37, 30, 33, 5, 26, 35, 5, 18, 5, 3...\n",
       "15    [0, 36, 34, 37, 30, 33, 5, 26, 35, 5, 35, 32, ...\n",
       "16    [0, 19, 18, 31, 31, 32, 31, 5, 26, 35, 5, 36, ...\n",
       "17    [0, 35, 18, 21, 29, 41, 10, 5, 4, 15, 5, 30, 2...\n",
       "18    [0, 35, 29, 22, 22, 33, 5, 36, 26, 24, 25, 36,...\n",
       "19    [0, 18, 5, 33, 34, 22, 29, 37, 21, 22, 5, 36, ...\n",
       "21    [0, 27, 32, 22, 5, 20, 18, 31, 5, 41, 32, 37, ...\n",
       "22    [0, 27, 22, 35, 37, 35, 5, 20, 25, 34, 26, 35,...\n",
       "23    [0, 30, 34, 12, 5, 33, 34, 22, 35, 26, 21, 22,...\n",
       "24    [0, 33, 34, 32, 19, 18, 19, 29, 41, 5, 24, 32,...\n",
       "25    [0, 28, 22, 22, 33, 5, 37, 33, 5, 36, 25, 22, ...\n",
       "26    [0, 32, 19, 38, 26, 32, 37, 35, 29, 41, 5, 41,...\n",
       "27    [0, 31, 32, 39, 5, 24, 32, 5, 21, 32, 5, 41, 3...\n",
       "28    [0, 36, 25, 22, 31, 5, 26, 5, 39, 26, 29, 29, ...\n",
       "30    [0, 21, 32, 5, 31, 32, 36, 5, 34, 22, 29, 41, ...\n",
       "31    [0, 36, 25, 26, 35, 5, 30, 18, 41, 5, 19, 22, ...\n",
       "                            ...                        \n",
       "37    [0, 30, 20, 20, 18, 26, 31, 8, 35, 5, 20, 32, ...\n",
       "38    [0, 27, 32, 25, 31, 5, 30, 20, 20, 18, 26, 31,...\n",
       "39    [0, 35, 22, 31, 18, 36, 32, 34, 5, 30, 20, 20,...\n",
       "40    [0, 32, 25, 5, 35, 25, 37, 36, 5, 37, 33, 5, 2...\n",
       "43    [0, 30, 20, 20, 18, 26, 31, 8, 35, 5, 29, 22, ...\n",
       "44    [0, 36, 25, 22, 5, 32, 31, 22, 5, 25, 22, 5, 3...\n",
       "45    [0, 39, 18, 28, 22, 5, 37, 33, 5, 18, 31, 21, ...\n",
       "46    [0, 30, 20, 20, 18, 26, 31, 5, 26, 35, 5, 18, ...\n",
       "47    [0, 35, 33, 26, 29, 29, 26, 31, 24, 5, 36, 25,...\n",
       "48    [0, 36, 34, 37, 30, 33, 8, 35, 5, 34, 37, 29, ...\n",
       "49    [0, 28, 22, 22, 33, 5, 35, 18, 38, 26, 31, 24,...\n",
       "54    [0, 36, 25, 22, 34, 22, 5, 18, 34, 22, 5, 21, ...\n",
       "55    [0, 41, 32, 37, 5, 20, 18, 31, 5, 4, 37, 26, 3...\n",
       "56    [0, 24, 32, 5, 23, 26, 29, 22, 5, 19, 18, 31, ...\n",
       "59    [0, 26, 5, 18, 30, 5, 31, 32, 36, 5, 34, 37, 3...\n",
       "61    [0, 41, 32, 37, 5, 34, 22, 18, 29, 29, 41, 5, ...\n",
       "63    [0, 19, 32, 36, 25, 5, 33, 18, 34, 36, 26, 22,...\n",
       "65    [0, 41, 32, 37, 5, 35, 36, 37, 33, 26, 21, 5, ...\n",
       "66    [0, 32, 19, 18, 30, 18, 5, 18, 31, 21, 5, 25, ...\n",
       "67    [0, 36, 34, 37, 30, 33, 5, 25, 18, 35, 5, 36, ...\n",
       "74    [0, 39, 25, 18, 36, 5, 26, 35, 5, 41, 32, 37, ...\n",
       "76    [0, 26, 5, 31, 32, 5, 29, 32, 31, 24, 22, 34, ...\n",
       "77    [0, 37, 36, 36, 22, 34, 29, 41, 5, 19, 29, 18,...\n",
       "78    [0, 18, 31, 41, 5, 37, 35, 5, 33, 34, 22, 35, ...\n",
       "79    [0, 26, 5, 31, 22, 38, 22, 34, 5, 36, 25, 32, ...\n",
       "83    [0, 35, 32, 5, 31, 32, 39, 5, 26, 36, 5, 19, 3...\n",
       "85    [0, 36, 25, 22, 5, 33, 18, 36, 34, 26, 32, 36,...\n",
       "86    [0, 39, 22, 5, 20, 18, 31, 31, 32, 36, 5, 29, ...\n",
       "87    [0, 29, 22, 36, 8, 35, 5, 31, 32, 36, 5, 33, 3...\n",
       "88    [0, 36, 25, 22, 34, 22, 5, 26, 35, 5, 31, 32, ...\n",
       "Name: comment_ints, Length: 64, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered[comment_int_colname].iloc[0:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### HM-LSTM #######\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, batch_size, J, V, emb_dim, hidden_dim, output_emb_dim, num_layers, \n",
    "                 grad_clip, sampling):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        if sampling == True:\n",
    "            batch_size, J = 1, 1\n",
    "        else:\n",
    "            batch_size, J = batch_size, J\n",
    "        \n",
    "        self.V = V\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_emb_dim = output_emb_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.learning_rate = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "        self.slope_annealing_placeholder = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "        self.xs = tf.placeholder(tf.int32, [batch_size, J])\n",
    "        self.ys = tf.placeholder(tf.int32, [batch_size, J])\n",
    "        \n",
    "        self.emb_initializer = tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "        self.emb_mat = tf.get_variable(\n",
    "            name='emb_mat', dtype=tf.float32, shape=[self.V, self.emb_dim],\n",
    "            initializer=self.emb_initializer\n",
    "        )\n",
    "        self.emb_xs = tf.nn.embedding_lookup(self.emb_mat, self.xs)\n",
    "        \n",
    "        #self.emb_xs = tf.one_hot(self.xs, self.V)\n",
    "\n",
    "        self.multi_cell = Multi_HM_LSTM_Cell(\n",
    "            [HM_LSTM_Cell(\n",
    "                num_units=self.hidden_dim, \n",
    "                slope_annealing_placeholder=self.slope_annealing_placeholder,\n",
    "                forget_bias=1.0)\n",
    "             for _ in range(0, self.num_layers)]\n",
    "        )\n",
    "        '''\n",
    "        self.multi_cell = tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.contrib.rnn.BasicLSTMCell(num_units=self.hidden_dim, forget_bias=1.0) \n",
    "             for _ in range(0, self.num_layers)]\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        self.initial_state = self.multi_cell.zero_state(batch_size, tf.float32)            \n",
    "        self.outputs, self.state = tf.nn.dynamic_rnn(\n",
    "            cell=self.multi_cell, inputs=self.emb_xs, initial_state=self.initial_state)\n",
    "\n",
    "        self.h_layer1, self.h_layer2, self.h_layer3 = self.outputs\n",
    "\n",
    "        print(\"each h_layer output should have shape [batch_size, timesteps, hidden dim]\")\n",
    "        print(self.h_layer1.get_shape().as_list())\n",
    "        print(self.h_layer2.get_shape().as_list())\n",
    "        print(self.h_layer3.get_shape().as_list())\n",
    "\n",
    "        self.h_layer1_per_char = tf.reshape(self.h_layer1, [-1, self.hidden_dim])\n",
    "        self.h_layer2_per_char = tf.reshape(self.h_layer2, [-1, self.hidden_dim])\n",
    "        self.h_layer3_per_char = tf.reshape(self.h_layer3, [-1, self.hidden_dim])\n",
    "\n",
    "        h_out_per_char = tf.concat(\n",
    "            [self.h_layer1_per_char, self.h_layer2_per_char, self.h_layer3_per_char], 1)\n",
    "        \n",
    "        g1 = tf.layers.dense(h_out_per_char, units=1, use_bias=False, activation=tf.nn.sigmoid)\n",
    "        g2 = tf.layers.dense(h_out_per_char, units=1, use_bias=False, activation=tf.nn.sigmoid)\n",
    "        g3 = tf.layers.dense(h_out_per_char, units=1, use_bias=False, activation=tf.nn.sigmoid)\n",
    "\n",
    "        self.output_emb = tf.layers.dense(\n",
    "            tf.concat([\n",
    "                g1 * self.h_layer1_per_char, \n",
    "                g2 * self.h_layer2_per_char, \n",
    "                g3 * self.h_layer3_per_char\n",
    "            ], 1), \n",
    "            units=self.output_emb_dim, \n",
    "            use_bias=False,\n",
    "            activation=None\n",
    "        )\n",
    "        self.output_emb = tf.maximum(0.10 * self.output_emb, self.output_emb)\n",
    "        \n",
    "        with tf.variable_scope('logit_layer'):\n",
    "            logits_kernel = tf.get_variable(name='logits_kernel', \n",
    "                shape=[self.output_emb_dim, self.V])\n",
    "    \n",
    "        self.logits = tf.matmul(self.output_emb, logits_kernel)\n",
    "        self.probabilities = tf.nn.softmax(self.logits)\n",
    "\n",
    "        self.loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels=self.ys,\n",
    "            logits=tf.reshape(self.logits, self.ys.get_shape().as_list() + [V])\n",
    "        )\n",
    "\n",
    "        #self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        tvars = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        gradients, _ = zip(*optimizer.compute_gradients(self.loss, tvars))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, grad_clip)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each h_layer output should have shape [batch_size, timesteps, hidden dim]\n",
      "[64, 100, 512]\n",
      "[64, 100, 512]\n",
      "[64, 100, 512]\n"
     ]
    }
   ],
   "source": [
    "model = Model(batch_size, J, V, emb_dim, hidden_dim, output_emb_dim, num_layers, \n",
    "    grad_clip, sampling=False)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 125... step 0...\t training loss: 3.7579402923583984\n",
      "Epoch 0 / 125... step 1...\t training loss: 3.672825813293457\n",
      "Epoch 0 / 125... step 2...\t training loss: 3.5305662155151367\n",
      "Epoch 0 / 125... step 3...\t training loss: 5.38958740234375\n",
      "Epoch 0 / 125... step 4...\t training loss: 8.181283950805664\n",
      "Epoch 0 / 125... step 5...\t training loss: 4.687193393707275\n",
      "Epoch 0 / 125... step 6...\t training loss: 2.1547369956970215\n",
      "Epoch 0 / 125... step 7...\t training loss: 2.261915445327759\n",
      "Epoch 0 / 125... step 8...\t training loss: 2.1636273860931396\n",
      "Epoch 0 / 125... step 9...\t training loss: 2.331103801727295\n",
      "Epoch 0 / 125... step 10...\t training loss: 1.9706655740737915\n",
      "Epoch 0 / 125... step 11...\t training loss: 1.8968554735183716\n",
      "Epoch 0 / 125... step 12...\t training loss: 1.970605731010437\n",
      "Epoch 0 / 125... step 13...\t training loss: 2.0801782608032227\n",
      "Epoch 0 / 125... step 14...\t training loss: 1.9405460357666016\n",
      "Epoch 0 / 125... step 15...\t training loss: 1.9388964176177979\n",
      "Epoch 0 / 125... step 16...\t training loss: 1.8123043775558472\n",
      "Epoch 0 / 125... step 17...\t training loss: 1.7861562967300415\n",
      "Epoch 0 / 125... step 18...\t training loss: 1.7543565034866333\n",
      "Epoch 0 / 125... step 19...\t training loss: 1.8320024013519287\n",
      "Epoch 0 / 125... step 20...\t training loss: 1.766538381576538\n",
      "Epoch 0 / 125... step 21...\t training loss: 1.7409594058990479\n",
      "Epoch 0 / 125... step 22...\t training loss: 1.8808236122131348\n",
      "Epoch 0 / 125... step 23...\t training loss: 1.6903057098388672\n",
      "Epoch 0 / 125... step 24...\t training loss: 1.8584023714065552\n",
      "Epoch 0 / 125... step 25...\t training loss: 1.5821301937103271\n",
      "Epoch 0 / 125... step 26...\t training loss: 1.660781979560852\n",
      "Epoch 0 / 125... step 27...\t training loss: 1.559088110923767\n",
      "Epoch 0 / 125... step 28...\t training loss: 1.6411007642745972\n",
      "Epoch 0 / 125... step 29...\t training loss: 1.6442080736160278\n",
      "Epoch 0 / 125... step 30...\t training loss: 1.6967915296554565\n",
      "Epoch 0 / 125... step 31...\t training loss: 1.8125884532928467\n",
      "Epoch 0 / 125... step 32...\t training loss: 1.5528392791748047\n",
      "Epoch 0 / 125... step 33...\t training loss: 1.6376705169677734\n",
      "Epoch 0 / 125... step 34...\t training loss: 1.5166102647781372\n",
      "Epoch 0 / 125... step 35...\t training loss: 1.6277122497558594\n",
      "Epoch 0 / 125... step 36...\t training loss: 1.5792653560638428\n",
      "Epoch 0 / 125... step 37...\t training loss: 1.4969837665557861\n",
      "Epoch 0 / 125... step 38...\t training loss: 1.716901183128357\n",
      "Epoch 0 / 125... step 39...\t training loss: 1.69125235080719\n",
      "Epoch 0 / 125... step 40...\t training loss: 1.5286887884140015\n",
      "Epoch 0 / 125... step 41...\t training loss: 1.5577442646026611\n",
      "Epoch 0 / 125... step 42...\t training loss: 2.288525342941284\n",
      "Epoch 0 / 125... step 43...\t training loss: 1.6541911363601685\n",
      "Epoch 0 / 125... step 44...\t training loss: 1.5959572792053223\n",
      "Epoch 0 / 125... step 45...\t training loss: 1.722247838973999\n",
      "Epoch 0 / 125... step 46...\t training loss: 1.5771050453186035\n",
      "Epoch 0 / 125... step 47...\t training loss: 1.5823605060577393\n",
      "Epoch 0 / 125... step 48...\t training loss: 1.588774561882019\n",
      "Epoch 0 / 125... step 49...\t training loss: 1.6333279609680176\n",
      "Epoch 0 / 125... step 50...\t training loss: 1.6469216346740723\n",
      "Epoch 0 / 125... step 51...\t training loss: 1.5336202383041382\n",
      "Epoch 0 / 125... step 52...\t training loss: 1.5844171047210693\n",
      "Epoch 0 / 125... step 53...\t training loss: 1.5405257940292358\n",
      "Epoch 0 / 125... step 54...\t training loss: 1.5636577606201172\n",
      "Epoch 0 / 125... step 55...\t training loss: 1.6319242715835571\n",
      "Epoch 0 / 125... step 56...\t training loss: 1.6414247751235962\n",
      "Epoch 0 / 125... step 57...\t training loss: 1.4553619623184204\n",
      "Epoch 0 / 125... step 58...\t training loss: 1.4263131618499756\n",
      "Epoch 0 / 125... step 59...\t training loss: 1.6777727603912354\n",
      "Epoch 0 / 125... step 60...\t training loss: 1.5184550285339355\n",
      "Epoch 0 / 125... step 61...\t training loss: 1.6836403608322144\n",
      "Epoch 0 / 125... step 62...\t training loss: 1.4821646213531494\n",
      "Epoch 0 / 125... step 63...\t training loss: 1.578985571861267\n",
      "Epoch 0 / 125... step 64...\t training loss: 1.6886730194091797\n",
      "Epoch 0 / 125... step 65...\t training loss: 1.4876253604888916\n",
      "Epoch 0 / 125... step 66...\t training loss: 1.54154634475708\n",
      "Epoch 0 / 125... step 67...\t training loss: 1.5026293992996216\n",
      "Epoch 0 / 125... step 68...\t training loss: 1.4810892343521118\n",
      "Epoch 0 / 125... step 69...\t training loss: 1.4479119777679443\n",
      "Epoch 0 / 125... step 70...\t training loss: 1.4122788906097412\n",
      "Epoch 0 / 125... step 71...\t training loss: 1.4691811800003052\n",
      "Epoch 0 / 125... step 72...\t training loss: 1.46562922000885\n",
      "Epoch 0 / 125... step 73...\t training loss: 1.4208015203475952\n",
      "Epoch 0 / 125... step 74...\t training loss: 1.6025866270065308\n",
      "Epoch 0 / 125... step 75...\t training loss: 1.5519921779632568\n",
      "Epoch 0 / 125... step 76...\t training loss: 1.4405008554458618\n",
      "Epoch 0 / 125... step 77...\t training loss: 1.4873464107513428\n",
      "Epoch 0 / 125... step 78...\t training loss: 1.4708383083343506\n",
      "Epoch 0 / 125... step 79...\t training loss: 1.6177407503128052\n",
      "Epoch 0 / 125... step 80...\t training loss: 1.5675699710845947\n",
      "Epoch 0 / 125... step 81...\t training loss: 1.4852142333984375\n",
      "Epoch 0 / 125... step 82...\t training loss: 1.4486771821975708\n",
      "Epoch 1 / 125... step 0...\t training loss: 1.521096110343933\n",
      "Epoch 1 / 125... step 1...\t training loss: 1.46310555934906\n",
      "Epoch 1 / 125... step 2...\t training loss: 1.5615155696868896\n",
      "Epoch 1 / 125... step 3...\t training loss: 1.5190480947494507\n",
      "Epoch 1 / 125... step 4...\t training loss: 1.4978017807006836\n",
      "Epoch 1 / 125... step 5...\t training loss: 1.427071213722229\n",
      "Epoch 1 / 125... step 6...\t training loss: 1.3686795234680176\n",
      "Epoch 1 / 125... step 7...\t training loss: 1.5994951725006104\n",
      "Epoch 1 / 125... step 8...\t training loss: 1.408199429512024\n",
      "Epoch 1 / 125... step 9...\t training loss: 1.382942795753479\n",
      "Epoch 1 / 125... step 10...\t training loss: 1.354021430015564\n",
      "Epoch 1 / 125... step 11...\t training loss: 1.4055395126342773\n",
      "Epoch 1 / 125... step 12...\t training loss: 1.278067708015442\n",
      "Epoch 1 / 125... step 13...\t training loss: 1.2790212631225586\n",
      "Epoch 1 / 125... step 14...\t training loss: 1.4115115404129028\n",
      "Epoch 1 / 125... step 15...\t training loss: 1.3071560859680176\n",
      "Epoch 1 / 125... step 16...\t training loss: 1.419496774673462\n",
      "Epoch 1 / 125... step 17...\t training loss: 1.5264660120010376\n",
      "Epoch 1 / 125... step 18...\t training loss: 1.2625693082809448\n",
      "Epoch 1 / 125... step 19...\t training loss: 1.4142006635665894\n",
      "Epoch 1 / 125... step 20...\t training loss: 1.415974736213684\n",
      "Epoch 1 / 125... step 21...\t training loss: 1.3532905578613281\n",
      "Epoch 1 / 125... step 22...\t training loss: 1.3894075155258179\n",
      "Epoch 1 / 125... step 23...\t training loss: 1.3054522275924683\n",
      "Epoch 1 / 125... step 24...\t training loss: 1.4551211595535278\n",
      "Epoch 1 / 125... step 25...\t training loss: 1.318577527999878\n",
      "Epoch 1 / 125... step 26...\t training loss: 1.3840020895004272\n",
      "Epoch 1 / 125... step 27...\t training loss: 1.4357054233551025\n",
      "Epoch 1 / 125... step 28...\t training loss: 1.397531509399414\n",
      "Epoch 1 / 125... step 29...\t training loss: 1.4579029083251953\n",
      "Epoch 1 / 125... step 30...\t training loss: 1.4129986763000488\n",
      "Epoch 1 / 125... step 31...\t training loss: 1.4071629047393799\n",
      "Epoch 1 / 125... step 32...\t training loss: 1.5025917291641235\n",
      "Epoch 1 / 125... step 33...\t training loss: 1.4829598665237427\n",
      "Epoch 1 / 125... step 34...\t training loss: 1.344482660293579\n",
      "Epoch 1 / 125... step 35...\t training loss: 1.339855432510376\n",
      "Epoch 1 / 125... step 36...\t training loss: 1.3948612213134766\n",
      "Epoch 1 / 125... step 37...\t training loss: 1.4440776109695435\n",
      "Epoch 1 / 125... step 38...\t training loss: 1.329817771911621\n",
      "Epoch 1 / 125... step 39...\t training loss: 1.3855462074279785\n",
      "Epoch 1 / 125... step 40...\t training loss: 1.5019934177398682\n",
      "Epoch 1 / 125... step 41...\t training loss: 1.396946668624878\n",
      "Epoch 1 / 125... step 42...\t training loss: 1.413828730583191\n",
      "Epoch 1 / 125... step 43...\t training loss: 1.4330390691757202\n",
      "Epoch 1 / 125... step 44...\t training loss: 1.361796498298645\n",
      "Epoch 1 / 125... step 45...\t training loss: 1.4298263788223267\n",
      "Epoch 1 / 125... step 46...\t training loss: 1.3567200899124146\n",
      "Epoch 1 / 125... step 47...\t training loss: 1.3197987079620361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 125... step 48...\t training loss: 1.3399581909179688\n",
      "Epoch 1 / 125... step 49...\t training loss: 1.3892285823822021\n",
      "Epoch 1 / 125... step 50...\t training loss: 1.417523741722107\n",
      "Epoch 1 / 125... step 51...\t training loss: 1.358782172203064\n",
      "Epoch 1 / 125... step 52...\t training loss: 1.3883548974990845\n",
      "Epoch 1 / 125... step 53...\t training loss: 1.3688204288482666\n",
      "Epoch 1 / 125... step 54...\t training loss: 1.2653846740722656\n",
      "Epoch 1 / 125... step 55...\t training loss: 1.2888848781585693\n",
      "Epoch 1 / 125... step 56...\t training loss: 1.2563011646270752\n",
      "Epoch 1 / 125... step 57...\t training loss: 1.32978355884552\n",
      "Epoch 1 / 125... step 58...\t training loss: 1.3307737112045288\n",
      "Epoch 1 / 125... step 59...\t training loss: 1.3456006050109863\n",
      "Epoch 1 / 125... step 60...\t training loss: 1.2946584224700928\n",
      "Epoch 1 / 125... step 61...\t training loss: 1.3899832963943481\n",
      "Epoch 1 / 125... step 62...\t training loss: 1.2682607173919678\n",
      "Epoch 1 / 125... step 63...\t training loss: 1.310361623764038\n",
      "Epoch 1 / 125... step 64...\t training loss: 1.403916597366333\n",
      "Epoch 1 / 125... step 65...\t training loss: 1.2989760637283325\n",
      "Epoch 1 / 125... step 66...\t training loss: 1.4477068185806274\n",
      "Epoch 1 / 125... step 67...\t training loss: 1.2548279762268066\n",
      "Epoch 1 / 125... step 68...\t training loss: 1.2953567504882812\n",
      "Epoch 1 / 125... step 69...\t training loss: 1.404679536819458\n",
      "Epoch 1 / 125... step 70...\t training loss: 1.2567329406738281\n",
      "Epoch 1 / 125... step 71...\t training loss: 1.2612824440002441\n",
      "Epoch 1 / 125... step 72...\t training loss: 1.2322428226470947\n",
      "Epoch 1 / 125... step 73...\t training loss: 1.3755563497543335\n",
      "Epoch 1 / 125... step 74...\t training loss: 1.195587396621704\n",
      "Epoch 1 / 125... step 75...\t training loss: 1.3333083391189575\n",
      "Epoch 1 / 125... step 76...\t training loss: 1.2961583137512207\n",
      "Epoch 1 / 125... step 77...\t training loss: 1.384534478187561\n",
      "Epoch 1 / 125... step 78...\t training loss: 1.2194634675979614\n",
      "Epoch 1 / 125... step 79...\t training loss: 1.363037109375\n",
      "Epoch 1 / 125... step 80...\t training loss: 1.3114194869995117\n",
      "Epoch 1 / 125... step 81...\t training loss: 1.2693623304367065\n",
      "Epoch 1 / 125... step 82...\t training loss: 1.2521326541900635\n",
      "Epoch 2 / 125... step 0...\t training loss: 1.3497408628463745\n",
      "Epoch 2 / 125... step 1...\t training loss: 1.2952297925949097\n",
      "Epoch 2 / 125... step 2...\t training loss: 1.3809231519699097\n",
      "Epoch 2 / 125... step 3...\t training loss: 1.2769442796707153\n",
      "Epoch 2 / 125... step 4...\t training loss: 1.2035586833953857\n",
      "Epoch 2 / 125... step 5...\t training loss: 1.273519515991211\n",
      "Epoch 2 / 125... step 6...\t training loss: 1.39085054397583\n",
      "Epoch 2 / 125... step 7...\t training loss: 1.2924401760101318\n",
      "Epoch 2 / 125... step 8...\t training loss: 1.340031385421753\n",
      "Epoch 2 / 125... step 9...\t training loss: 1.2894599437713623\n",
      "Epoch 2 / 125... step 10...\t training loss: 1.3436927795410156\n",
      "Epoch 2 / 125... step 11...\t training loss: 1.1919699907302856\n",
      "Epoch 2 / 125... step 12...\t training loss: 1.2594395875930786\n",
      "Epoch 2 / 125... step 13...\t training loss: 1.1318976879119873\n",
      "Epoch 2 / 125... step 14...\t training loss: 1.4487661123275757\n",
      "Epoch 2 / 125... step 15...\t training loss: 1.2155348062515259\n",
      "Epoch 2 / 125... step 16...\t training loss: 1.1968002319335938\n",
      "Epoch 2 / 125... step 17...\t training loss: 1.4554721117019653\n",
      "Epoch 2 / 125... step 18...\t training loss: 1.2411859035491943\n",
      "Epoch 2 / 125... step 19...\t training loss: 1.2801439762115479\n",
      "Epoch 2 / 125... step 20...\t training loss: 1.3011291027069092\n",
      "Epoch 2 / 125... step 21...\t training loss: 1.2789074182510376\n",
      "Epoch 2 / 125... step 22...\t training loss: 1.3398537635803223\n",
      "Epoch 2 / 125... step 23...\t training loss: 1.1868233680725098\n",
      "Epoch 2 / 125... step 24...\t training loss: 1.2675217390060425\n",
      "Epoch 2 / 125... step 25...\t training loss: 1.3271325826644897\n",
      "Epoch 2 / 125... step 26...\t training loss: 1.2979769706726074\n",
      "Epoch 2 / 125... step 27...\t training loss: 1.1542167663574219\n",
      "Epoch 2 / 125... step 28...\t training loss: 1.246712565422058\n",
      "Epoch 2 / 125... step 29...\t training loss: 1.1808969974517822\n",
      "Epoch 2 / 125... step 30...\t training loss: 1.2309426069259644\n",
      "Epoch 2 / 125... step 31...\t training loss: 1.1476072072982788\n",
      "Epoch 2 / 125... step 32...\t training loss: 1.2378231287002563\n",
      "Epoch 2 / 125... step 33...\t training loss: 1.224205493927002\n",
      "Epoch 2 / 125... step 34...\t training loss: 1.2020821571350098\n",
      "Epoch 2 / 125... step 35...\t training loss: 1.1345932483673096\n",
      "Epoch 2 / 125... step 36...\t training loss: 1.1429998874664307\n",
      "Epoch 2 / 125... step 37...\t training loss: 1.21918785572052\n",
      "Epoch 2 / 125... step 38...\t training loss: 1.229931116104126\n",
      "Epoch 2 / 125... step 39...\t training loss: 1.1776856184005737\n",
      "Epoch 2 / 125... step 40...\t training loss: 1.2071088552474976\n",
      "Epoch 2 / 125... step 41...\t training loss: 1.173439860343933\n",
      "Epoch 2 / 125... step 42...\t training loss: 1.1043506860733032\n",
      "Epoch 2 / 125... step 43...\t training loss: 1.2543585300445557\n",
      "Epoch 2 / 125... step 44...\t training loss: 1.1367509365081787\n",
      "Epoch 2 / 125... step 45...\t training loss: 1.1826496124267578\n",
      "Epoch 2 / 125... step 46...\t training loss: 1.3018720149993896\n",
      "Epoch 2 / 125... step 47...\t training loss: 1.1237373352050781\n",
      "Epoch 2 / 125... step 48...\t training loss: 1.2182400226593018\n",
      "Epoch 2 / 125... step 49...\t training loss: 1.2065491676330566\n",
      "Epoch 2 / 125... step 50...\t training loss: 1.232021450996399\n",
      "Epoch 2 / 125... step 51...\t training loss: 1.2462732791900635\n",
      "Epoch 2 / 125... step 52...\t training loss: 1.2955704927444458\n",
      "Epoch 2 / 125... step 53...\t training loss: 1.2132312059402466\n",
      "Epoch 2 / 125... step 54...\t training loss: 1.2643306255340576\n",
      "Epoch 2 / 125... step 55...\t training loss: 1.2277065515518188\n",
      "Epoch 2 / 125... step 56...\t training loss: 1.2075366973876953\n",
      "Epoch 2 / 125... step 57...\t training loss: 1.1609123945236206\n",
      "Epoch 2 / 125... step 58...\t training loss: 1.1258354187011719\n",
      "Epoch 2 / 125... step 59...\t training loss: 1.1309176683425903\n",
      "Epoch 2 / 125... step 60...\t training loss: 1.2614535093307495\n",
      "Epoch 2 / 125... step 61...\t training loss: 1.308402419090271\n",
      "Epoch 2 / 125... step 62...\t training loss: 1.1572132110595703\n",
      "Epoch 2 / 125... step 63...\t training loss: 1.2364330291748047\n",
      "Epoch 2 / 125... step 64...\t training loss: 1.2479336261749268\n",
      "Epoch 2 / 125... step 65...\t training loss: 1.1016820669174194\n",
      "Epoch 2 / 125... step 66...\t training loss: 1.1538575887680054\n",
      "Epoch 2 / 125... step 67...\t training loss: 1.133218765258789\n",
      "Epoch 2 / 125... step 68...\t training loss: 1.1959320306777954\n",
      "Epoch 2 / 125... step 69...\t training loss: 1.0959224700927734\n",
      "Epoch 2 / 125... step 70...\t training loss: 1.1731715202331543\n",
      "Epoch 2 / 125... step 71...\t training loss: 1.0705004930496216\n",
      "Epoch 2 / 125... step 72...\t training loss: 1.205074429512024\n",
      "Epoch 2 / 125... step 73...\t training loss: 1.1869922876358032\n",
      "Epoch 2 / 125... step 74...\t training loss: 1.1826748847961426\n",
      "Epoch 2 / 125... step 75...\t training loss: 1.2117940187454224\n",
      "Epoch 2 / 125... step 76...\t training loss: 1.1109570264816284\n"
     ]
    }
   ],
   "source": [
    "learning_rate = init_learning_rate\n",
    "slope_value = init_slope_value\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    \n",
    "    if epoch > 0 and (epoch % learning_rate_epochs_per_annealing == 0):\n",
    "        \n",
    "        learning_rate = max(\n",
    "            minimum_learning_rate, \n",
    "            learning_rate * learning_rate_annealing_const)\n",
    "    \n",
    "    if epoch > 0:\n",
    "        slope_value = min(\n",
    "            max_slope, \n",
    "            slope_value + slope_annealing_increase_per_epoch)\n",
    "    \n",
    "    df_filtered = df_filtered.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    for i in range(0, dataset_size // batch_size):\n",
    "        batch_idx = i * batch_size\n",
    "        \n",
    "        start_idx = batch_idx\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        df_batch = df_filtered.iloc[start_idx:end_idx]\n",
    "        comment_ints_batch = df_batch[comment_int_colname].tolist()\n",
    "        comment_ints_batch = np.array(comment_ints_batch, dtype=np.int32)\n",
    "\n",
    "        xs_batch = comment_ints_batch[:,0:J]\n",
    "        ys_batch = comment_ints_batch[:,1:(J+1)]\n",
    "\n",
    "        feed_dict = {model.xs: xs_batch, \n",
    "                     model.ys: ys_batch, \n",
    "                     model.slope_annealing_placeholder: slope_value, \n",
    "                     model.learning_rate: learning_rate}\n",
    "\n",
    "        _, loss_batch, rnn_outputs_, probs_, h_layer1_, hlayer1_tf_reshaped_ = sess.run(\n",
    "            [model.train_op, model.loss, model.outputs, model.probabilities, \n",
    "             model.h_layer1, model.h_layer1_per_char], \n",
    "            feed_dict=feed_dict\n",
    "        )\n",
    "        \n",
    "        print(\"Epoch {} / {}... step {}...\\t training loss: {}\".format(\n",
    "                epoch, num_epochs, i, loss_batch))\n",
    "        '''\n",
    "        if i % 10 == 0:\n",
    "            temp_df = pd.DataFrame()\n",
    "            temp_df['comments'] = df_filtered[standardized_text_colname].iloc[start_idx:end_idx]\n",
    "            #temp_df['rnn_outputs'] = [np.array(row) for row in np.array(rnn_outputs_[2]).tolist()]\n",
    "            #probs_ = np.reshape(probs_, [-1, J, V])\n",
    "            #print(probs_[0:5, :, 0:4])\n",
    "            #print(temp_df.head())\n",
    "            \n",
    "            print(h_layer1_[0:5, 0, 0])\n",
    "            # ^ for first five batch items \n",
    "            # show me the model's lowest layer state \n",
    "            # at vector location zero \n",
    "            # immediately after the first go token \n",
    "            # in each of the 5 sequences\n",
    "            # the values should all match (very important!)\n",
    "            \n",
    "            print(hlayer1_tf_reshaped_[0, 0])  \n",
    "            print(hlayer1_tf_reshaped_[100, 0])\n",
    "            print(hlayer1_tf_reshaped_[200, 0])\n",
    "            print(hlayer1_tf_reshaped_[300, 0])\n",
    "            print(hlayer1_tf_reshaped_[400, 0])\n",
    "            # ^and reshaping shouldnt screw anything up\n",
    "            \n",
    "            print(\"*\" * 80)\n",
    "            \n",
    "            print(\"\\nthese should all match: \")\n",
    "            print(probs_[0, 0])\n",
    "            print(probs_[100, 0])\n",
    "            print(probs_[200, 0])\n",
    "            print(probs_[300, 0])\n",
    "            print(probs_[400, 0])\n",
    "            # ^these should all match \n",
    "            \n",
    "            print(\"\\nand reshaping shouldnt screw anything up\")\n",
    "            probs_np_reshaped_ = np.reshape(probs_, [batch_size, J, V])\n",
    "            print(probs_np_reshaped_[0:5, 0, 0])\n",
    "        '''\n",
    "            \n",
    "    if epoch > 0 and (epoch % 2 == 0):\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir = 'checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/hm_lstm_L3_h350_e6.ckpt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# e = num_epochs\n",
    "e = 6 \n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "saver.save(sess, os.path.join(checkpoint_dir, \"hm_lstm_L{}_h{}_e{}.ckpt\".format(\n",
    "                    num_layers, hidden_dim, e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, nr_chars, top_n=4):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(nr_chars, 1, p=p)[0]\n",
    "    return c\n",
    "\n",
    "CONTROL_CHARS = [go, end_of_text, pad, end_of_padded_comment]\n",
    "\n",
    "def sample(checkpoint, J, hidden_dim, V, prime=\"The \"):\n",
    "    \n",
    "    samples = [c for c in prime]\n",
    "    model = Model(batch_size, J, V, emb_dim, hidden_dim, output_emb_dim, num_layers, \n",
    "                  grad_clip, sampling=False)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        zero_state = sess.run(model.initial_state)\n",
    "        \n",
    "        prime = ''.join([go] + list(prime))\n",
    "        \n",
    "        x = np.zeros((batch_size, J))\n",
    "        \n",
    "        for i in range(0,len(prime)):\n",
    "            c = prime[i]\n",
    "            x[:,i] = np.array([char2int[c] for _ in range(0, batch_size)])\n",
    "            feed = {model.xs: x,\n",
    "                    model.slope_annealing_placeholder: 5.0,\n",
    "                    model.initial_state: zero_state}\n",
    "            preds_ch, new_state = sess.run(\n",
    "                [model.probabilities, model.state], \n",
    "                feed_dict=feed)\n",
    "            \n",
    "            preds_ch = np.reshape(preds_ch, [batch_size, J, V])[0, i, :]\n",
    "            char_id = pick_top_n(preds_ch, nr_chars=V, top_n=4)\n",
    "            \n",
    "        if int2char[char_id] in CONTROL_CHARS:\n",
    "            return ''.join(samples)\n",
    "        else:\n",
    "            samples.append(int2char[char_id])\n",
    "\n",
    "        for i in range(len(prime), J):\n",
    "            x[:,i] = np.array([char_id for _ in range(0, batch_size)])\n",
    "            feed = {model.xs: x,\n",
    "                    model.slope_annealing_placeholder: 5.0,\n",
    "                    model.initial_state: zero_state}\n",
    "            preds_ch, new_state = sess.run(\n",
    "                [model.probabilities, model.state], \n",
    "                feed_dict=feed)\n",
    "                \n",
    "            preds_ch = np.reshape(preds_ch, [batch_size, J, V])[0, i, :]\n",
    "            char_id = pick_top_n(preds_ch, nr_chars=V, top_n=2)\n",
    "            if int2char[char_id] in CONTROL_CHARS:\n",
    "                break\n",
    "            else:\n",
    "                samples.append(int2char[char_id])\n",
    "    \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/hm_lstm_L3_h350_e6.ckpt'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each h_layer output should have shape [batch_size, timesteps, hidden dim]\n",
      "[64, 100, 350]\n",
      "[64, 100, 350]\n",
      "[64, 100, 350]\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/hm_lstm_L3_h350_e6.ckpt\n"
     ]
    }
   ],
   "source": [
    "samp = sample(checkpoint, 100, hidden_dim, V, prime=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Toee     eee e    ttt      '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each h_layer output should have shape [batch_size, timesteps, hidden dim]\n",
      "[64, 100, 350]\n",
      "[64, 100, 350]\n",
      "[64, 100, 350]\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/hm_lstm_L3_h350_e6.ckpt\n"
     ]
    }
   ],
   "source": [
    "samp = sample(checkpoint, 100, hidden_dim, V, prime=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Toe     e    e e   ttttt   '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
